{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Contents<a id='4.1_Contents'></a>\n",
    "* [4 Modeling](#4_Modeling)\n",
    "  * [4.1 Contents](#4.1_Contents)\n",
    "  * [4.2 Introduction](#4.2_Introduction)\n",
    "  * [4.3 Imports](#4.3_Imports)\n",
    "  * [4.4 Load The Data](#4.4_Load_The_Data)\n",
    "    * [4.4.1 Explore The Data](#4.4.1_Explore_The_Data)\n",
    "    * [4.4.2 Fixing Categorical Data](#4.4.2_Fixing_Data)\n",
    "  * [4.5 Train/Test_Split](#4.5_Train/Test_Split)\n",
    "  * [4.6 Initial Average Mode](#4.6_Average_Model)\n",
    "  * [4.7 Metrics](#4.7_Metrics)\n",
    "  * [4.8 Imputations](#4.8_Imputations)\n",
    "  * [4.9 Dimensitionality Reduction](#4.9_Dimensionality)\n",
    "      * [4.9.1 Manual Imputation](#4.9.1_Manual_Imputations)   \n",
    "  * [4.10 Gradient Boosting Model](#4.10_Gradient_Boosting)\n",
    "  * [4.11 Hyper-parameter Tuning Gradient Boost](#4.11_Hyper_parameter_Tuning)\n",
    "  * [4.12 Best Model](#4.12_Best_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>\n",
    "#### In this notebook we take our preproccessed data from our Preprocessing notebook and try different models to see which yield the best performance and tune our hyper-parameters to maximize best results. Quick disclaimer the data used in the prior notebooks have been superseded by a new dataframe, while the dataset is the same I went back and redid my final dataset which has more observation and a different set of features. I felt that while the world I did originally was fine, but I needed to test few parameters and different features so I will be using a new feature set but for the most part everything else is the same. I also implement a MICE imputation method rather than just imputation of mean values for each feature set\n",
    "\n",
    "#### With the goal to make the best model which predicts house prices in the NYC market for capital fortune we have done a lot of data work cleaning and wrangling a very messy zillow dataset with various missing values and messy feature set. But after some tedious work we were able to establish a workable dataset to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.ticker as tick\n",
    "import sklearn.model_selection\n",
    "\n",
    "from operator import itemgetter\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier)\n",
    "import xgboost as xgb\n",
    "\n",
    "import featuretools as ft\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
