{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Contents<a id='4.1_Contents'></a>\n",
    "* [4 Modeling](#4_Modeling)\n",
    "  * [4.1 Contents](#4.1_Contents)\n",
    "  * [4.2 Introduction](#4.2_Introduction)\n",
    "  * [4.3 Imports](#4.3_Imports)\n",
    "  * [4.4 Load The Data](#4.4_Load_The_Data)\n",
    "  * [4.5 Initial Average Mode](#4.5_Average_Model)\n",
    "  * [4.6 Metrics](#4.7_Metrics)\n",
    "  * [4.7 Models](#4.7_Models)\n",
    "      * [4.7.1 Linear Model](#4.7.1_Linear_Model)    \n",
    "      * [4.7.2 Nearest Neighbor](#4.7.2_Nearest_Neighbor)    \n",
    "      * [4.7.3 RandomForest](#4.7.3_RandomForest)    \n",
    "      * [4.7.4 GradientBoosting](#4.7.4_Gradient)    \n",
    "  * [4.8 Hyper-parameter Tuning Gradient Boost](#4.11_Hyper_parameter_Tuning)\n",
    "  * [4.9 Best Model](#4.9_Best_Model)\n",
    "  * [4.10 Summary](#4.10_Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>\n",
    "#### In this notebook we take our preproccessed data from our Preprocessing notebook and try different models to see which yield the best performance and tune our hyper-parameters to maximize best results. Quick disclaimer the data used in the prior notebooks have been superseded by a new dataframe, while the dataset is the same I went back and redid my final dataset which has more observation and a different set of features. I felt that while the world I did originally was fine, but I needed to test few parameters and different features so I will be using a new feature set but for the most part everything else is the same. I also implement a MICE imputation method rather than just imputation of mean values for each feature set\n",
    "\n",
    "#### With the goal to make the best model which predicts house prices in the NYC market for capital fortune we have done a lot of data work cleaning and wrangling a very messy zillow dataset with various missing values and messy feature set. But after some tedious work we were able to establish a workable dataset to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.ticker as tick\n",
    "import sklearn.model_selection\n",
    "\n",
    "from operator import itemgetter\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier)\n",
    "import xgboost as xgb\n",
    "\n",
    "import featuretools as ft\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "\n",
    "class style:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is pulled from scikit learn source I do not own this code\n",
    "from sklearn.utils.validation import check_consistent_length, check_array\n",
    "\n",
    "def mape(y_true, y_pred,\n",
    "                                   sample_weight=None,\n",
    "                                   multioutput='uniform_average'):\n",
    "    \"\"\"Mean absolute percentage error regression loss.\n",
    "    Note here that we do not represent the output as a percentage in range\n",
    "    [0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the\n",
    "    :ref:`User Guide <mean_absolute_percentage_error>`.\n",
    "    .. versionadded:: 0.24\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "    multioutput : {'raw_values', 'uniform_average'} or array-like\n",
    "        Defines aggregating of multiple output values.\n",
    "        Array-like value defines weights used to average errors.\n",
    "        If input is list then the shape must be (n_outputs,).\n",
    "        'raw_values' :\n",
    "            Returns a full set of errors in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Errors of all outputs are averaged with uniform weight.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float or ndarray of floats in the range [0, 1/eps]\n",
    "        If multioutput is 'raw_values', then mean absolute percentage error\n",
    "        is returned for each output separately.\n",
    "        If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
    "        weighted average of all output errors is returned.\n",
    "        MAPE output is non-negative floating point. The best value is 0.0.\n",
    "        But note the fact that bad predictions can lead to arbitarily large\n",
    "        MAPE values, especially if some y_true values are very close to zero.\n",
    "        Note that we return a large value instead of `inf` when y_true is zero.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.metrics import mean_absolute_percentage_error\n",
    "    >>> y_true = [3, -0.5, 2, 7]\n",
    "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
    "    >>> mean_absolute_percentage_error(y_true, y_pred)\n",
    "    0.3273...\n",
    "    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "    >>> mean_absolute_percentage_error(y_true, y_pred)\n",
    "    0.5515...\n",
    "    >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "    0.6198...\n",
    "    \"\"\"\n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "    output_errors = np.average(mape,\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\n",
    "    \"\"\"Check that y_true and y_pred belong to the same regression task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "    y_pred : array-like\n",
    "    multioutput : array-like or string in ['raw_values', uniform_average',\n",
    "        'variance_weighted'] or None\n",
    "        None is accepted due to backward compatibility of r2_score().\n",
    "    Returns\n",
    "    -------\n",
    "    type_true : one of {'continuous', continuous-multioutput'}\n",
    "        The type of the true target data, as output by\n",
    "        'utils.multiclass.type_of_target'.\n",
    "    y_true : array-like of shape (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    multioutput : array-like of shape (n_outputs) or string in ['raw_values',\n",
    "        uniform_average', 'variance_weighted'] or None\n",
    "        Custom output weights if ``multioutput`` is array-like or\n",
    "        just the corresponding argument if ``multioutput`` is a\n",
    "        correct keyword.\n",
    "    dtype : str or list, default=\"numeric\"\n",
    "        the dtype argument passed to check_array.\n",
    "    \"\"\"\n",
    "    check_consistent_length(y_true, y_pred)\n",
    "    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n",
    "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape((-1, 1))\n",
    "\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "    if y_true.shape[1] != y_pred.shape[1]:\n",
    "        raise ValueError(\"y_true and y_pred have different number of output \"\n",
    "                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n",
    "\n",
    "    n_outputs = y_true.shape[1]\n",
    "    allowed_multioutput_str = ('raw_values', 'uniform_average',\n",
    "                               'variance_weighted')\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput not in allowed_multioutput_str:\n",
    "            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\n",
    "                             \"You provided multioutput={!r}\".format(\n",
    "                                 allowed_multioutput_str,\n",
    "                                 multioutput))\n",
    "    elif multioutput is not None:\n",
    "        multioutput = check_array(multioutput, ensure_2d=False)\n",
    "        if n_outputs == 1:\n",
    "            raise ValueError(\"Custom weights are useful only in \"\n",
    "                             \"multi-output cases.\")\n",
    "        elif n_outputs != len(multioutput):\n",
    "            raise ValueError((\"There must be equally many custom weights \"\n",
    "                              \"(%d) as outputs (%d).\") %\n",
    "                             (len(multioutput), n_outputs))\n",
    "    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\n",
    "\n",
    "    return y_type, y_true, y_pred, multioutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Load The Data<a id='4.4_Load_The_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All of our data is imputed for their missing values utlizing the MICE imputation and, categorical values have been encoding to numbers, they are also not ordinal. As mentioned earlier few adjustments were made with features as few of school features were included as they had a positive impact on MAE reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>livingArea</th>\n",
       "      <th>longitude</th>\n",
       "      <th>priceChangeRate</th>\n",
       "      <th>propertyTaxRate</th>\n",
       "      <th>HomeType</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Full_Bathrooms</th>\n",
       "      <th>Half_Bathrooms</th>\n",
       "      <th>...</th>\n",
       "      <th>Annual_Tax</th>\n",
       "      <th>Tax_Assessed_Value</th>\n",
       "      <th>school_1_distance</th>\n",
       "      <th>school_1_rating</th>\n",
       "      <th>school_1_size</th>\n",
       "      <th>school_1_s/t_ratio</th>\n",
       "      <th>school_2_distance</th>\n",
       "      <th>school_2_rating</th>\n",
       "      <th>school_2_size</th>\n",
       "      <th>school_2_s/t_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.154509</td>\n",
       "      <td>-1.724627</td>\n",
       "      <td>0.858494</td>\n",
       "      <td>-2.301475</td>\n",
       "      <td>-0.026462</td>\n",
       "      <td>0.447383</td>\n",
       "      <td>0.322011</td>\n",
       "      <td>1.794010</td>\n",
       "      <td>0.165641</td>\n",
       "      <td>0.086021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068242</td>\n",
       "      <td>0.057307</td>\n",
       "      <td>3.087951</td>\n",
       "      <td>0.329303</td>\n",
       "      <td>-0.662430</td>\n",
       "      <td>-0.476408</td>\n",
       "      <td>0.529444</td>\n",
       "      <td>1.949000</td>\n",
       "      <td>0.123038</td>\n",
       "      <td>-0.146034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.646870</td>\n",
       "      <td>-0.775620</td>\n",
       "      <td>-0.134432</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>-0.026439</td>\n",
       "      <td>-1.811992</td>\n",
       "      <td>-0.919517</td>\n",
       "      <td>-1.181012</td>\n",
       "      <td>-0.686736</td>\n",
       "      <td>-0.030701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071497</td>\n",
       "      <td>-0.289346</td>\n",
       "      <td>-0.272991</td>\n",
       "      <td>-1.099026</td>\n",
       "      <td>-0.023647</td>\n",
       "      <td>0.496564</td>\n",
       "      <td>0.129008</td>\n",
       "      <td>-0.303900</td>\n",
       "      <td>-0.657712</td>\n",
       "      <td>-0.146034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629343</td>\n",
       "      <td>-0.266081</td>\n",
       "      <td>0.475534</td>\n",
       "      <td>-0.201490</td>\n",
       "      <td>-0.026369</td>\n",
       "      <td>-1.811992</td>\n",
       "      <td>-0.919517</td>\n",
       "      <td>-0.874309</td>\n",
       "      <td>0.165641</td>\n",
       "      <td>-0.078314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076955</td>\n",
       "      <td>0.724850</td>\n",
       "      <td>-0.578531</td>\n",
       "      <td>-0.622916</td>\n",
       "      <td>-0.978460</td>\n",
       "      <td>-0.476408</td>\n",
       "      <td>-0.805345</td>\n",
       "      <td>0.146680</td>\n",
       "      <td>-1.247349</td>\n",
       "      <td>-2.674455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.846814</td>\n",
       "      <td>1.863459</td>\n",
       "      <td>-0.572493</td>\n",
       "      <td>0.483297</td>\n",
       "      <td>-0.026454</td>\n",
       "      <td>1.268973</td>\n",
       "      <td>-2.161045</td>\n",
       "      <td>1.579318</td>\n",
       "      <td>-1.229475</td>\n",
       "      <td>-0.078314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068559</td>\n",
       "      <td>0.102184</td>\n",
       "      <td>-0.578531</td>\n",
       "      <td>-0.622916</td>\n",
       "      <td>0.615136</td>\n",
       "      <td>0.496564</td>\n",
       "      <td>-0.538387</td>\n",
       "      <td>-1.655639</td>\n",
       "      <td>-0.659193</td>\n",
       "      <td>-0.146034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.393703</td>\n",
       "      <td>0.550104</td>\n",
       "      <td>-1.033406</td>\n",
       "      <td>0.108092</td>\n",
       "      <td>-0.026462</td>\n",
       "      <td>0.139286</td>\n",
       "      <td>0.942775</td>\n",
       "      <td>-0.720957</td>\n",
       "      <td>-0.531917</td>\n",
       "      <td>0.086021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077763</td>\n",
       "      <td>0.603266</td>\n",
       "      <td>0.032549</td>\n",
       "      <td>0.805413</td>\n",
       "      <td>0.961423</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>-0.137950</td>\n",
       "      <td>1.047840</td>\n",
       "      <td>-0.395487</td>\n",
       "      <td>-0.146034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ZipCode  latitude  livingArea  longitude  priceChangeRate  \\\n",
       "0 -1.154509 -1.724627    0.858494  -2.301475        -0.026462   \n",
       "1  0.646870 -0.775620   -0.134432   0.003659        -0.026439   \n",
       "2  0.629343 -0.266081    0.475534  -0.201490        -0.026369   \n",
       "3 -0.846814  1.863459   -0.572493   0.483297        -0.026454   \n",
       "4  0.393703  0.550104   -1.033406   0.108092        -0.026462   \n",
       "\n",
       "   propertyTaxRate  HomeType  YearBuilt  Full_Bathrooms  Half_Bathrooms  ...  \\\n",
       "0         0.447383  0.322011   1.794010        0.165641        0.086021  ...   \n",
       "1        -1.811992 -0.919517  -1.181012       -0.686736       -0.030701  ...   \n",
       "2        -1.811992 -0.919517  -0.874309        0.165641       -0.078314  ...   \n",
       "3         1.268973 -2.161045   1.579318       -1.229475       -0.078314  ...   \n",
       "4         0.139286  0.942775  -0.720957       -0.531917        0.086021  ...   \n",
       "\n",
       "   Annual_Tax  Tax_Assessed_Value  school_1_distance  school_1_rating  \\\n",
       "0   -0.068242            0.057307           3.087951         0.329303   \n",
       "1   -0.071497           -0.289346          -0.272991        -1.099026   \n",
       "2   -0.076955            0.724850          -0.578531        -0.622916   \n",
       "3   -0.068559            0.102184          -0.578531        -0.622916   \n",
       "4   -0.077763            0.603266           0.032549         0.805413   \n",
       "\n",
       "   school_1_size  school_1_s/t_ratio  school_2_distance  school_2_rating  \\\n",
       "0      -0.662430           -0.476408           0.529444         1.949000   \n",
       "1      -0.023647            0.496564           0.129008        -0.303900   \n",
       "2      -0.978460           -0.476408          -0.805345         0.146680   \n",
       "3       0.615136            0.496564          -0.538387        -1.655639   \n",
       "4       0.961423            0.010078          -0.137950         1.047840   \n",
       "\n",
       "   school_2_size  school_2_s/t_ratio  \n",
       "0       0.123038           -0.146034  \n",
       "1      -0.657712           -0.146034  \n",
       "2      -1.247349           -2.674455  \n",
       "3      -0.659193           -0.146034  \n",
       "4      -0.395487           -0.146034  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(ss.fit_transform(X_train),columns=X_train.columns,index=X_train.index)\n",
    "X_test = pd.DataFrame(ss.transform(X_test),columns=X_test.columns,index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Average Model<a id='4.5_Average_Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Y_train Average prediction's: 496524.566655386\n",
      "MAE of Y_test Average prediction's: 495842.7715914151\n"
     ]
    }
   ],
   "source": [
    "train_mean = [y_train.mean()] * len(y_train)\n",
    "test_mean = [y_train.mean()] * len(y_test)\n",
    "\n",
    "print(\"MAE of Y_train Average prediction's:\",mean_absolute_error(y_train, train_mean))\n",
    "print(\"MAE of Y_test Average prediction's:\",mean_absolute_error(y_test, test_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Metrics<a id='4.6_Metrics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Metrics used will be     \n",
    "     - R²- R² shows how well terms (data points) fit a curve or line.\n",
    "     - MAE -Mean absolute error\n",
    "     - MSE -Mean squared error\n",
    "     - RMSE- Root mean squared error. It is the square root of the MSE.\n",
    "     - Mean Absolute Percentage Error (MAPE)\n",
    "     \n",
    "Ideally you like to see all the metrics improve with each respective metric but that is not always the case for that reason we will be using MAE as the main metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Models<a id='4.7_Models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1 Linear Model<a id='4.7.1_Linear_Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.3476670695998434\n",
      "\u001b[1mMAE  379598.388210031\u001b[0m\n",
      "MSE  532111095846.1924\n",
      "RMSE  616.115564005675\n",
      "MAPE  0.5354992708284384\n"
     ]
    }
   ],
   "source": [
    "print('R2 ',r2(y_test, lr_pred))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, lr_pred)) + style.END)\n",
    "print('MSE ',mse(y_test, lr_pred))\n",
    "print('RMSE ',np.sqrt(mae(y_test, lr_pred)))\n",
    "print('MAPE ',(mape(y_test, lr_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 Nearest Neighbor<a id='4.7.2_Nearest_Neighbor'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.577790505087675\n",
      "MAE  261900.7622478386\n",
      "MSE  344398307282.5273\n",
      "RMSE  511.762408005745\n",
      "RMSLE  0.4104330214552605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "nn =  KNeighborsRegressor(n_neighbors=2)\n",
    "nn.fit(X_train, y_train)\n",
    "nn_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.577790505087675\n",
      "\u001b[1mMAE  261900.7622478386\u001b[0m\n",
      "MSE  344398307282.5273\n",
      "RMSE  511.762408005745\n",
      "MAPE  0.31505120655524266\n"
     ]
    }
   ],
   "source": [
    "print('R2 ',r2(y_test, nn_pred))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, nn_pred)) + style.END)\n",
    "print('MSE ',mse(y_test, nn_pred))\n",
    "print('RMSE ',np.sqrt(mae(y_test, nn_pred)))\n",
    "print('MAPE ',mape(y_test, nn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets do some parameter tuning to find the best parameter n_neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = np.arange(1,50)\n",
    "gridsearch_params = {'n_neighbors': gk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "k = np.random.randint(1,50,25)\n",
    "estimator_KNN = KNeighborsRegressor()\n",
    "parameters_KNN = {\n",
    "    'n_neighbors': k\n",
    "\n",
    "}\n",
    "\n",
    "random_search_KNN = RandomizedSearchCV(\n",
    "    estimator=estimator_KNN,\n",
    "    param_distributions=parameters_KNN,\n",
    "    n_iter=5,\n",
    "    scoring= 'neg_mean_absolute_error',\n",
    "    n_jobs= -1,\n",
    "    cv= 5\n",
    ")\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_rs = random_search_KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 18}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.6243890936885258\n",
      "\u001b[1mMAE  249954.90951542323\u001b[0m\n",
      "MSE  306387615364.7142\n",
      "RMSE  499.95490748208806\n",
      "MAPE  0.32169259875363854\n"
     ]
    }
   ],
   "source": [
    "nn =  KNeighborsRegressor(n_neighbors=18)\n",
    "nn.fit(X_train, y_train)\n",
    "nn_pred = nn.predict(X_test)\n",
    "\n",
    "print('R2 ',r2(y_test, nn_pred))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, nn_pred)) + style.END)\n",
    "print('MSE ',mse(y_test, nn_pred))\n",
    "print('RMSE ',np.sqrt(mae(y_test, nn_pred)))\n",
    "print('MAPE ',mape(y_test, nn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 RandomForest<a id='4.7.3_RandomForest'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=100, max_depth=10)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.6750328848517655\n",
      "\u001b[1mMAE  236077.92307150294\u001b[0m\n",
      "MSE  265077232341.20724\n",
      "RMSE  485.87850649262407\n",
      "MAPE  0.32627741299599256\n"
     ]
    }
   ],
   "source": [
    "print('R2 ',r2(y_test, rf_pred))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, rf_pred)) + style.END)\n",
    "print('MSE ',mse(y_test, rf_pred))\n",
    "print('RMSE ',np.sqrt(mae(y_test, rf_pred)))\n",
    "print('MAPE ',mape(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(50,150,10)\n",
    "md = np.arange(5,15)\n",
    "\n",
    "estimator_RF = RandomForestRegressor()\n",
    "parameters_RF = {\n",
    "    'n_estimators': k,\n",
    "    'max_depth' : md\n",
    "}\n",
    "\n",
    "random_search_RF = RandomizedSearchCV(\n",
    "    estimator=estimator_RF,\n",
    "    param_distributions=parameters_RF,\n",
    "    n_iter=5,\n",
    "    scoring= 'neg_mean_absolute_error',\n",
    "    n_jobs= -1,\n",
    "    cv= 5\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 80, 'max_depth': 14}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rs = random_search_RF.fit(X_train, y_train)\n",
    "rf_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.6979017175517661\n",
      "\u001b[1mMAE  222413.3680726387\u001b[0m\n",
      "MSE  246423015971.57526\n",
      "RMSE  471.60721800311615\n",
      "MAPE  0.3014211033041731\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=80, max_depth=14)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_pred = rf.predict(X_test)\n",
    "print('R2 ',r2(y_test, rf_pred))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, rf_pred)) + style.END)\n",
    "print('MSE ',mse(y_test, rf_pred))\n",
    "print('RMSE ',np.sqrt(mae(y_test, rf_pred)))\n",
    "print('MAPE ',mape(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.4 Gradient Boosting<a id='4.7.4_Gradient'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = xgb.XGBRegressor()\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "gb_train = gb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  0.6965447340625471\n",
      "\u001b[1mMAE  220371.97907164492\u001b[0m\n",
      "MSE  247529913903.35767\n",
      "RMSE  469.4379395315689\n",
      "MAPE  0.28959049413396326\n"
     ]
    }
   ],
   "source": [
    "print('R2 ',r2(y_test, gb_pred))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, gb_pred)) + style.END)\n",
    "print('MSE ',mse(y_test, gb_pred))\n",
    "print('RMSE ',np.sqrt(mae(y_test, gb_pred)))\n",
    "print('MAPE ',mape(y_test, gb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From all the models it seems Gradient Boosting had the best results as for as MAE now lets go further into more hyper-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Hyperparameter Tuning<a id='4.11_Hyper_parameter_Tuning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From all the models we got the best results from XGBoost's gradient boost and thus we will attempt to try to squeeze better results. We will first make a log-transformation of our target variable of 'price', this will help with outliers or the extremes of very high priced homes which don't contribute to majority of the distribution. Then we will proceed to tune the parameters of XGBoost in our case it will be\n",
    "    - eta which is the learning rate of our model which is the shrinkage you do at every step you are making, increasing this allows for much faster computation where as decreasing this allows for best optimum result\n",
    "    - max_depth controls how complex a model becomes, more depth more likely it is to over-fit where too little leads to simple and underfitting model\n",
    "    - min_child_weight is how much weight is placed on each leaf node, the larger it is the more conservative the algorithm will be.\n",
    "    - subsample is the ratio of training instances, this is the amount of observations to be randomly samples for each tree. Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - colsample_bytree this is the same as subsample except this is for the columns in other words the features \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "\n",
    "log_y_train = np.log(y_train)\n",
    "log_y_test = np.log(y_test)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=log_y_train, feature_names=feature_names)\n",
    "\n",
    "dtest = xgb.DMatrix(X_test, label=log_y_test, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "}\n",
    "\n",
    "params['eval_metric'] = \"mae\"\n",
    "num_boost_round = 999\n",
    "evallist  = [(X_test,'eval'), (X_train,'train')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:9.10071\n",
      "[1]\tTest-mae:6.37143\n",
      "[2]\tTest-mae:4.46099\n",
      "[3]\tTest-mae:3.12300\n",
      "[4]\tTest-mae:2.18704\n",
      "[5]\tTest-mae:1.53318\n",
      "[6]\tTest-mae:1.08051\n",
      "[7]\tTest-mae:0.77355\n",
      "[8]\tTest-mae:0.56891\n",
      "[9]\tTest-mae:0.43379\n",
      "[10]\tTest-mae:0.34782\n",
      "[11]\tTest-mae:0.29544\n",
      "[12]\tTest-mae:0.26577\n",
      "[13]\tTest-mae:0.24856\n",
      "[14]\tTest-mae:0.23821\n",
      "[15]\tTest-mae:0.23288\n",
      "[16]\tTest-mae:0.22937\n",
      "[17]\tTest-mae:0.22704\n",
      "[18]\tTest-mae:0.22574\n",
      "[19]\tTest-mae:0.22404\n",
      "[20]\tTest-mae:0.22281\n",
      "[21]\tTest-mae:0.22155\n",
      "[22]\tTest-mae:0.22113\n",
      "[23]\tTest-mae:0.22043\n",
      "[24]\tTest-mae:0.21978\n",
      "[25]\tTest-mae:0.21945\n",
      "[26]\tTest-mae:0.21843\n",
      "[27]\tTest-mae:0.21797\n",
      "[28]\tTest-mae:0.21758\n",
      "[29]\tTest-mae:0.21740\n",
      "[30]\tTest-mae:0.21690\n",
      "[31]\tTest-mae:0.21637\n",
      "[32]\tTest-mae:0.21634\n",
      "[33]\tTest-mae:0.21602\n",
      "[34]\tTest-mae:0.21545\n",
      "[35]\tTest-mae:0.21504\n",
      "[36]\tTest-mae:0.21479\n",
      "[37]\tTest-mae:0.21464\n",
      "[38]\tTest-mae:0.21465\n",
      "[39]\tTest-mae:0.21454\n",
      "[40]\tTest-mae:0.21401\n",
      "[41]\tTest-mae:0.21388\n",
      "[42]\tTest-mae:0.21353\n",
      "[43]\tTest-mae:0.21331\n",
      "[44]\tTest-mae:0.21322\n",
      "[45]\tTest-mae:0.21306\n",
      "[46]\tTest-mae:0.21273\n",
      "[47]\tTest-mae:0.21254\n",
      "[48]\tTest-mae:0.21249\n",
      "[49]\tTest-mae:0.21253\n",
      "[50]\tTest-mae:0.21237\n",
      "[51]\tTest-mae:0.21238\n",
      "[52]\tTest-mae:0.21210\n",
      "[53]\tTest-mae:0.21177\n",
      "[54]\tTest-mae:0.21142\n",
      "[55]\tTest-mae:0.21144\n",
      "[56]\tTest-mae:0.21128\n",
      "[57]\tTest-mae:0.21127\n",
      "[58]\tTest-mae:0.21120\n",
      "[59]\tTest-mae:0.21114\n",
      "[60]\tTest-mae:0.21102\n",
      "[61]\tTest-mae:0.21059\n",
      "[62]\tTest-mae:0.21027\n",
      "[63]\tTest-mae:0.21026\n",
      "[64]\tTest-mae:0.21000\n",
      "[65]\tTest-mae:0.20994\n",
      "[66]\tTest-mae:0.20976\n",
      "[67]\tTest-mae:0.20951\n",
      "[68]\tTest-mae:0.20937\n",
      "[69]\tTest-mae:0.20935\n",
      "[70]\tTest-mae:0.20938\n",
      "[71]\tTest-mae:0.20935\n",
      "[72]\tTest-mae:0.20935\n",
      "[73]\tTest-mae:0.20930\n",
      "[74]\tTest-mae:0.20927\n",
      "[75]\tTest-mae:0.20909\n",
      "[76]\tTest-mae:0.20889\n",
      "[77]\tTest-mae:0.20902\n",
      "[78]\tTest-mae:0.20897\n",
      "[79]\tTest-mae:0.20888\n",
      "[80]\tTest-mae:0.20881\n",
      "[81]\tTest-mae:0.20872\n",
      "[82]\tTest-mae:0.20872\n",
      "[83]\tTest-mae:0.20871\n",
      "[84]\tTest-mae:0.20874\n",
      "[85]\tTest-mae:0.20856\n",
      "[86]\tTest-mae:0.20860\n",
      "[87]\tTest-mae:0.20851\n",
      "[88]\tTest-mae:0.20843\n",
      "[89]\tTest-mae:0.20832\n",
      "[90]\tTest-mae:0.20813\n",
      "[91]\tTest-mae:0.20806\n",
      "[92]\tTest-mae:0.20801\n",
      "[93]\tTest-mae:0.20806\n",
      "[94]\tTest-mae:0.20803\n",
      "[95]\tTest-mae:0.20802\n",
      "[96]\tTest-mae:0.20791\n",
      "[97]\tTest-mae:0.20780\n",
      "[98]\tTest-mae:0.20782\n",
      "[99]\tTest-mae:0.20784\n",
      "[100]\tTest-mae:0.20776\n",
      "[101]\tTest-mae:0.20768\n",
      "[102]\tTest-mae:0.20764\n",
      "[103]\tTest-mae:0.20763\n",
      "[104]\tTest-mae:0.20751\n",
      "[105]\tTest-mae:0.20745\n",
      "[106]\tTest-mae:0.20744\n",
      "[107]\tTest-mae:0.20737\n",
      "[108]\tTest-mae:0.20730\n",
      "[109]\tTest-mae:0.20727\n",
      "[110]\tTest-mae:0.20711\n",
      "[111]\tTest-mae:0.20710\n",
      "[112]\tTest-mae:0.20705\n",
      "[113]\tTest-mae:0.20702\n",
      "[114]\tTest-mae:0.20697\n",
      "[115]\tTest-mae:0.20692\n",
      "[116]\tTest-mae:0.20693\n",
      "[117]\tTest-mae:0.20683\n",
      "[118]\tTest-mae:0.20678\n",
      "[119]\tTest-mae:0.20675\n",
      "[120]\tTest-mae:0.20671\n",
      "[121]\tTest-mae:0.20664\n",
      "[122]\tTest-mae:0.20659\n",
      "[123]\tTest-mae:0.20662\n",
      "[124]\tTest-mae:0.20661\n",
      "[125]\tTest-mae:0.20659\n",
      "[126]\tTest-mae:0.20654\n",
      "[127]\tTest-mae:0.20651\n",
      "[128]\tTest-mae:0.20648\n",
      "[129]\tTest-mae:0.20660\n",
      "[130]\tTest-mae:0.20669\n",
      "[131]\tTest-mae:0.20667\n",
      "[132]\tTest-mae:0.20670\n",
      "[133]\tTest-mae:0.20673\n",
      "[134]\tTest-mae:0.20670\n",
      "[135]\tTest-mae:0.20669\n",
      "[136]\tTest-mae:0.20670\n",
      "[137]\tTest-mae:0.20661\n",
      "[138]\tTest-mae:0.20660\n",
      "Best MAE: 0.21 with 129 rounds\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(\"Best MAE: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-mae-mean</th>\n",
       "      <th>train-mae-std</th>\n",
       "      <th>test-mae-mean</th>\n",
       "      <th>test-mae-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.098022</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>9.097941</td>\n",
       "      <td>0.002037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.369637</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>6.369566</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.459765</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>4.459718</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.122816</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>3.122906</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.186898</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>2.187020</td>\n",
       "      <td>0.001763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.145527</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.206422</td>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.145283</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.206397</td>\n",
       "      <td>0.002371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.145076</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.206376</td>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.144823</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.206335</td>\n",
       "      <td>0.002440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.144561</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.206324</td>\n",
       "      <td>0.002455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
       "0          9.098022       0.000508       9.097941      0.002037\n",
       "1          6.369637       0.000308       6.369566      0.001374\n",
       "2          4.459765       0.000232       4.459718      0.001449\n",
       "3          3.122816       0.000170       3.122906      0.001572\n",
       "4          2.186898       0.000146       2.187020      0.001763\n",
       "..              ...            ...            ...           ...\n",
       "158        0.145527       0.001158       0.206422      0.002392\n",
       "159        0.145283       0.001136       0.206397      0.002371\n",
       "160        0.145076       0.001120       0.206376      0.002402\n",
       "161        0.144823       0.001132       0.206335      0.002440\n",
       "162        0.144561       0.001038       0.206324      0.002455\n",
       "\n",
       "[163 rows x 4 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=100,\n",
    "    nfold=5,\n",
    "    metrics={'mae'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(6,16)\n",
    "    for min_child_weight in range(3,7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=6, min_child_weight=3\n",
      "\tMAE 0.206074 for 154 rounds\n",
      "CV with max_depth=6, min_child_weight=4\n",
      "\tMAE 0.20579680000000003 for 185 rounds\n",
      "CV with max_depth=6, min_child_weight=5\n",
      "\tMAE 0.2063786 for 195 rounds\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "\tMAE 0.2056066 for 168 rounds\n",
      "CV with max_depth=7, min_child_weight=3\n",
      "\tMAE 0.2056016 for 191 rounds\n",
      "CV with max_depth=7, min_child_weight=4\n",
      "\tMAE 0.20527120000000001 for 151 rounds\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "\tMAE 0.20550100000000002 for 189 rounds\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "\tMAE 0.20660160000000002 for 136 rounds\n",
      "CV with max_depth=8, min_child_weight=3\n",
      "\tMAE 0.20601180000000002 for 127 rounds\n",
      "CV with max_depth=8, min_child_weight=4\n",
      "\tMAE 0.2052666 for 131 rounds\n",
      "CV with max_depth=8, min_child_weight=5\n",
      "\tMAE 0.2059428 for 129 rounds\n",
      "CV with max_depth=8, min_child_weight=6\n",
      "\tMAE 0.20563820000000002 for 134 rounds\n",
      "CV with max_depth=9, min_child_weight=3\n",
      "\tMAE 0.2050914 for 140 rounds\n",
      "CV with max_depth=9, min_child_weight=4\n",
      "\tMAE 0.2065434 for 106 rounds\n",
      "CV with max_depth=9, min_child_weight=5\n",
      "\tMAE 0.2061772 for 91 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tMAE 0.2055338 for 77 rounds\n",
      "CV with max_depth=10, min_child_weight=3\n",
      "\tMAE 0.20554059999999996 for 80 rounds\n",
      "CV with max_depth=10, min_child_weight=4\n",
      "\tMAE 0.2062208 for 74 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tMAE 0.2065188 for 114 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tMAE 0.207104 for 64 rounds\n",
      "CV with max_depth=11, min_child_weight=3\n",
      "\tMAE 0.20641760000000003 for 82 rounds\n",
      "CV with max_depth=11, min_child_weight=4\n",
      "\tMAE 0.20613699999999996 for 159 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tMAE 0.207342 for 70 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tMAE 0.2074662 for 57 rounds\n",
      "CV with max_depth=12, min_child_weight=3\n",
      "\tMAE 0.20623580000000002 for 183 rounds\n",
      "CV with max_depth=12, min_child_weight=4\n",
      "\tMAE 0.20640619999999998 for 80 rounds\n",
      "CV with max_depth=12, min_child_weight=5\n",
      "\tMAE 0.2077626 for 83 rounds\n",
      "CV with max_depth=12, min_child_weight=6\n",
      "\tMAE 0.20712199999999997 for 92 rounds\n",
      "CV with max_depth=13, min_child_weight=3\n",
      "\tMAE 0.207267 for 129 rounds\n",
      "CV with max_depth=13, min_child_weight=4\n",
      "\tMAE 0.20573480000000002 for 324 rounds\n",
      "CV with max_depth=13, min_child_weight=5\n",
      "\tMAE 0.2064852 for 105 rounds\n",
      "CV with max_depth=13, min_child_weight=6\n",
      "\tMAE 0.20796299999999998 for 47 rounds\n",
      "CV with max_depth=14, min_child_weight=3\n",
      "\tMAE 0.2061846 for 419 rounds\n",
      "CV with max_depth=14, min_child_weight=4\n",
      "\tMAE 0.20782099999999998 for 104 rounds\n",
      "CV with max_depth=14, min_child_weight=5\n",
      "\tMAE 0.207131 for 130 rounds\n",
      "CV with max_depth=14, min_child_weight=6\n",
      "\tMAE 0.2066208 for 82 rounds\n",
      "CV with max_depth=15, min_child_weight=3\n",
      "\tMAE 0.20713379999999998 for 348 rounds\n",
      "CV with max_depth=15, min_child_weight=4\n",
      "\tMAE 0.20738099999999998 for 402 rounds\n",
      "CV with max_depth=15, min_child_weight=5\n",
      "\tMAE 0.20797439999999998 for 55 rounds\n",
      "CV with max_depth=15, min_child_weight=6\n",
      "\tMAE 0.2069432 for 125 rounds\n",
      "Best params: 9, 3, MAE: 0.2050914\n"
     ]
    }
   ],
   "source": [
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=100,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = best_params[0]\n",
    "params['min_child_weight'] = best_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "\tMAE 0.2050914 for 140 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "\tMAE 0.20512420000000003 for 103 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tMAE 0.20504380000000003 for 160 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "\tMAE 0.20559279999999996 for 96 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tMAE 0.205681 for 137 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tMAE 0.20729199999999998 for 111 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tMAE 0.20687540000000001 for 74 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tMAE 0.2076512 for 115 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tMAE 0.20890880000000003 for 87 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tMAE 0.20985739999999997 for 70 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tMAE 0.20857739999999997 for 64 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tMAE 0.20909019999999998 for 82 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "\tMAE 0.21191 for 53 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tMAE 0.2117872 for 60 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tMAE 0.2110124 for 81 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tMAE 0.211496 for 52 rounds\n",
      "Best params: 1.0, 0.8, MAE: 0.20504380000000003\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=100,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = best_params[0]\n",
    "params['colsample_bytree'] = best_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "CV with eta=0.3\n",
      "Wall time: 0 ns\n",
      "\tMAE 0.20504380000000003 for 160 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "Wall time: 0 ns\n",
      "\tMAE 0.198117 for 359 rounds\n",
      "\n",
      "CV with eta=0.1\n",
      "Wall time: 0 ns\n",
      "\tMAE 0.19227979999999997 for 626 rounds\n",
      "\n",
      "CV with eta=0.05\n",
      "Wall time: 0 ns\n",
      "\tMAE 0.18994719999999998 for 998 rounds\n",
      "\n",
      "CV with eta=0.01\n",
      "Wall time: 0 ns\n",
      "\tMAE 0.1956408 for 998 rounds\n",
      "\n",
      "CV with eta=0.005\n",
      "Wall time: 0 ns\n",
      "\tMAE 0.22772920000000002 for 998 rounds\n",
      "\n",
      "Best params: 0.05, MAE: 0.18994719999999998\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# This can take some time…\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    # Run and time CV\n",
    "    %time \n",
    "    cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=100,\n",
    "            nfold=5,\n",
    "            metrics=['mae'],\n",
    "            early_stopping_rounds=10)\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9,\n",
       " 'min_child_weight': 3,\n",
       " 'eta': 0.05,\n",
       " 'subsample': 1.0,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'eval_metric': 'mae'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['eta'] = best_params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:12.34912\n",
      "[1]\tTest-mae:11.73199\n",
      "[2]\tTest-mae:11.14561\n",
      "[3]\tTest-mae:10.58857\n",
      "[4]\tTest-mae:10.05947\n",
      "[5]\tTest-mae:9.55672\n",
      "[6]\tTest-mae:9.07917\n",
      "[7]\tTest-mae:8.62550\n",
      "[8]\tTest-mae:8.19450\n",
      "[9]\tTest-mae:7.78502\n",
      "[10]\tTest-mae:7.39600\n",
      "[11]\tTest-mae:7.02643\n",
      "[12]\tTest-mae:6.67531\n",
      "[13]\tTest-mae:6.34183\n",
      "[14]\tTest-mae:6.02498\n",
      "[15]\tTest-mae:5.72384\n",
      "[16]\tTest-mae:5.43780\n",
      "[17]\tTest-mae:5.16606\n",
      "[18]\tTest-mae:4.90800\n",
      "[19]\tTest-mae:4.66278\n",
      "[20]\tTest-mae:4.42979\n",
      "[21]\tTest-mae:4.20846\n",
      "[22]\tTest-mae:3.99813\n",
      "[23]\tTest-mae:3.79838\n",
      "[24]\tTest-mae:3.60855\n",
      "[25]\tTest-mae:3.42815\n",
      "[26]\tTest-mae:3.25703\n",
      "[27]\tTest-mae:3.09434\n",
      "[28]\tTest-mae:2.93977\n",
      "[29]\tTest-mae:2.79286\n",
      "[30]\tTest-mae:2.65345\n",
      "[31]\tTest-mae:2.52101\n",
      "[32]\tTest-mae:2.39507\n",
      "[33]\tTest-mae:2.27558\n",
      "[34]\tTest-mae:2.16199\n",
      "[35]\tTest-mae:2.05416\n",
      "[36]\tTest-mae:1.95175\n",
      "[37]\tTest-mae:1.85456\n",
      "[38]\tTest-mae:1.76229\n",
      "[39]\tTest-mae:1.67478\n",
      "[40]\tTest-mae:1.59170\n",
      "[41]\tTest-mae:1.51293\n",
      "[42]\tTest-mae:1.43828\n",
      "[43]\tTest-mae:1.36731\n",
      "[44]\tTest-mae:1.30011\n",
      "[45]\tTest-mae:1.23631\n",
      "[46]\tTest-mae:1.17572\n",
      "[47]\tTest-mae:1.11830\n",
      "[48]\tTest-mae:1.06398\n",
      "[49]\tTest-mae:1.01254\n",
      "[50]\tTest-mae:0.96391\n",
      "[51]\tTest-mae:0.91787\n",
      "[52]\tTest-mae:0.87447\n",
      "[53]\tTest-mae:0.83340\n",
      "[54]\tTest-mae:0.79460\n",
      "[55]\tTest-mae:0.75782\n",
      "[56]\tTest-mae:0.72321\n",
      "[57]\tTest-mae:0.69033\n",
      "[58]\tTest-mae:0.65948\n",
      "[59]\tTest-mae:0.63043\n",
      "[60]\tTest-mae:0.60295\n",
      "[61]\tTest-mae:0.57714\n",
      "[62]\tTest-mae:0.55273\n",
      "[63]\tTest-mae:0.52965\n",
      "[64]\tTest-mae:0.50802\n",
      "[65]\tTest-mae:0.48763\n",
      "[66]\tTest-mae:0.46844\n",
      "[67]\tTest-mae:0.45049\n",
      "[68]\tTest-mae:0.43366\n",
      "[69]\tTest-mae:0.41792\n",
      "[70]\tTest-mae:0.40303\n",
      "[71]\tTest-mae:0.38917\n",
      "[72]\tTest-mae:0.37606\n",
      "[73]\tTest-mae:0.36394\n",
      "[74]\tTest-mae:0.35248\n",
      "[75]\tTest-mae:0.34180\n",
      "[76]\tTest-mae:0.33178\n",
      "[77]\tTest-mae:0.32233\n",
      "[78]\tTest-mae:0.31361\n",
      "[79]\tTest-mae:0.30544\n",
      "[80]\tTest-mae:0.29798\n",
      "[81]\tTest-mae:0.29093\n",
      "[82]\tTest-mae:0.28443\n",
      "[83]\tTest-mae:0.27833\n",
      "[84]\tTest-mae:0.27276\n",
      "[85]\tTest-mae:0.26743\n",
      "[86]\tTest-mae:0.26265\n",
      "[87]\tTest-mae:0.25820\n",
      "[88]\tTest-mae:0.25411\n",
      "[89]\tTest-mae:0.25026\n",
      "[90]\tTest-mae:0.24674\n",
      "[91]\tTest-mae:0.24349\n",
      "[92]\tTest-mae:0.24052\n",
      "[93]\tTest-mae:0.23778\n",
      "[94]\tTest-mae:0.23517\n",
      "[95]\tTest-mae:0.23282\n",
      "[96]\tTest-mae:0.23063\n",
      "[97]\tTest-mae:0.22860\n",
      "[98]\tTest-mae:0.22668\n",
      "[99]\tTest-mae:0.22500\n",
      "[100]\tTest-mae:0.22340\n",
      "[101]\tTest-mae:0.22199\n",
      "[102]\tTest-mae:0.22072\n",
      "[103]\tTest-mae:0.21944\n",
      "[104]\tTest-mae:0.21827\n",
      "[105]\tTest-mae:0.21716\n",
      "[106]\tTest-mae:0.21619\n",
      "[107]\tTest-mae:0.21530\n",
      "[108]\tTest-mae:0.21439\n",
      "[109]\tTest-mae:0.21363\n",
      "[110]\tTest-mae:0.21289\n",
      "[111]\tTest-mae:0.21219\n",
      "[112]\tTest-mae:0.21155\n",
      "[113]\tTest-mae:0.21102\n",
      "[114]\tTest-mae:0.21046\n",
      "[115]\tTest-mae:0.20989\n",
      "[116]\tTest-mae:0.20936\n",
      "[117]\tTest-mae:0.20894\n",
      "[118]\tTest-mae:0.20855\n",
      "[119]\tTest-mae:0.20809\n",
      "[120]\tTest-mae:0.20778\n",
      "[121]\tTest-mae:0.20740\n",
      "[122]\tTest-mae:0.20702\n",
      "[123]\tTest-mae:0.20675\n",
      "[124]\tTest-mae:0.20643\n",
      "[125]\tTest-mae:0.20617\n",
      "[126]\tTest-mae:0.20588\n",
      "[127]\tTest-mae:0.20567\n",
      "[128]\tTest-mae:0.20542\n",
      "[129]\tTest-mae:0.20516\n",
      "[130]\tTest-mae:0.20491\n",
      "[131]\tTest-mae:0.20463\n",
      "[132]\tTest-mae:0.20447\n",
      "[133]\tTest-mae:0.20425\n",
      "[134]\tTest-mae:0.20407\n",
      "[135]\tTest-mae:0.20390\n",
      "[136]\tTest-mae:0.20377\n",
      "[137]\tTest-mae:0.20364\n",
      "[138]\tTest-mae:0.20350\n",
      "[139]\tTest-mae:0.20328\n",
      "[140]\tTest-mae:0.20306\n",
      "[141]\tTest-mae:0.20285\n",
      "[142]\tTest-mae:0.20269\n",
      "[143]\tTest-mae:0.20258\n",
      "[144]\tTest-mae:0.20249\n",
      "[145]\tTest-mae:0.20240\n",
      "[146]\tTest-mae:0.20230\n",
      "[147]\tTest-mae:0.20222\n",
      "[148]\tTest-mae:0.20211\n",
      "[149]\tTest-mae:0.20204\n",
      "[150]\tTest-mae:0.20192\n",
      "[151]\tTest-mae:0.20183\n",
      "[152]\tTest-mae:0.20173\n",
      "[153]\tTest-mae:0.20159\n",
      "[154]\tTest-mae:0.20144\n",
      "[155]\tTest-mae:0.20131\n",
      "[156]\tTest-mae:0.20124\n",
      "[157]\tTest-mae:0.20117\n",
      "[158]\tTest-mae:0.20110\n",
      "[159]\tTest-mae:0.20096\n",
      "[160]\tTest-mae:0.20086\n",
      "[161]\tTest-mae:0.20082\n",
      "[162]\tTest-mae:0.20075\n",
      "[163]\tTest-mae:0.20067\n",
      "[164]\tTest-mae:0.20064\n",
      "[165]\tTest-mae:0.20054\n",
      "[166]\tTest-mae:0.20046\n",
      "[167]\tTest-mae:0.20036\n",
      "[168]\tTest-mae:0.20031\n",
      "[169]\tTest-mae:0.20020\n",
      "[170]\tTest-mae:0.20009\n",
      "[171]\tTest-mae:0.20000\n",
      "[172]\tTest-mae:0.19996\n",
      "[173]\tTest-mae:0.19994\n",
      "[174]\tTest-mae:0.19987\n",
      "[175]\tTest-mae:0.19982\n",
      "[176]\tTest-mae:0.19973\n",
      "[177]\tTest-mae:0.19964\n",
      "[178]\tTest-mae:0.19954\n",
      "[179]\tTest-mae:0.19942\n",
      "[180]\tTest-mae:0.19937\n",
      "[181]\tTest-mae:0.19931\n",
      "[182]\tTest-mae:0.19929\n",
      "[183]\tTest-mae:0.19925\n",
      "[184]\tTest-mae:0.19919\n",
      "[185]\tTest-mae:0.19913\n",
      "[186]\tTest-mae:0.19912\n",
      "[187]\tTest-mae:0.19910\n",
      "[188]\tTest-mae:0.19904\n",
      "[189]\tTest-mae:0.19895\n",
      "[190]\tTest-mae:0.19886\n",
      "[191]\tTest-mae:0.19876\n",
      "[192]\tTest-mae:0.19872\n",
      "[193]\tTest-mae:0.19868\n",
      "[194]\tTest-mae:0.19867\n",
      "[195]\tTest-mae:0.19856\n",
      "[196]\tTest-mae:0.19846\n",
      "[197]\tTest-mae:0.19842\n",
      "[198]\tTest-mae:0.19837\n",
      "[199]\tTest-mae:0.19830\n",
      "[200]\tTest-mae:0.19827\n",
      "[201]\tTest-mae:0.19824\n",
      "[202]\tTest-mae:0.19818\n",
      "[203]\tTest-mae:0.19813\n",
      "[204]\tTest-mae:0.19807\n",
      "[205]\tTest-mae:0.19804\n",
      "[206]\tTest-mae:0.19802\n",
      "[207]\tTest-mae:0.19801\n",
      "[208]\tTest-mae:0.19794\n",
      "[209]\tTest-mae:0.19788\n",
      "[210]\tTest-mae:0.19780\n",
      "[211]\tTest-mae:0.19776\n",
      "[212]\tTest-mae:0.19773\n",
      "[213]\tTest-mae:0.19769\n",
      "[214]\tTest-mae:0.19762\n",
      "[215]\tTest-mae:0.19756\n",
      "[216]\tTest-mae:0.19755\n",
      "[217]\tTest-mae:0.19752\n",
      "[218]\tTest-mae:0.19748\n",
      "[219]\tTest-mae:0.19739\n",
      "[220]\tTest-mae:0.19735\n",
      "[221]\tTest-mae:0.19732\n",
      "[222]\tTest-mae:0.19728\n",
      "[223]\tTest-mae:0.19728\n",
      "[224]\tTest-mae:0.19726\n",
      "[225]\tTest-mae:0.19722\n",
      "[226]\tTest-mae:0.19716\n",
      "[227]\tTest-mae:0.19714\n",
      "[228]\tTest-mae:0.19707\n",
      "[229]\tTest-mae:0.19705\n",
      "[230]\tTest-mae:0.19701\n",
      "[231]\tTest-mae:0.19699\n",
      "[232]\tTest-mae:0.19694\n",
      "[233]\tTest-mae:0.19691\n",
      "[234]\tTest-mae:0.19689\n",
      "[235]\tTest-mae:0.19680\n",
      "[236]\tTest-mae:0.19676\n",
      "[237]\tTest-mae:0.19673\n",
      "[238]\tTest-mae:0.19671\n",
      "[239]\tTest-mae:0.19670\n",
      "[240]\tTest-mae:0.19664\n",
      "[241]\tTest-mae:0.19663\n",
      "[242]\tTest-mae:0.19654\n",
      "[243]\tTest-mae:0.19650\n",
      "[244]\tTest-mae:0.19645\n",
      "[245]\tTest-mae:0.19641\n",
      "[246]\tTest-mae:0.19635\n",
      "[247]\tTest-mae:0.19634\n",
      "[248]\tTest-mae:0.19632\n",
      "[249]\tTest-mae:0.19630\n",
      "[250]\tTest-mae:0.19626\n",
      "[251]\tTest-mae:0.19624\n",
      "[252]\tTest-mae:0.19620\n",
      "[253]\tTest-mae:0.19616\n",
      "[254]\tTest-mae:0.19615\n",
      "[255]\tTest-mae:0.19612\n",
      "[256]\tTest-mae:0.19609\n",
      "[257]\tTest-mae:0.19607\n",
      "[258]\tTest-mae:0.19602\n",
      "[259]\tTest-mae:0.19595\n",
      "[260]\tTest-mae:0.19593\n",
      "[261]\tTest-mae:0.19588\n",
      "[262]\tTest-mae:0.19586\n",
      "[263]\tTest-mae:0.19584\n",
      "[264]\tTest-mae:0.19579\n",
      "[265]\tTest-mae:0.19577\n",
      "[266]\tTest-mae:0.19572\n",
      "[267]\tTest-mae:0.19572\n",
      "[268]\tTest-mae:0.19570\n",
      "[269]\tTest-mae:0.19564\n",
      "[270]\tTest-mae:0.19556\n",
      "[271]\tTest-mae:0.19554\n",
      "[272]\tTest-mae:0.19552\n",
      "[273]\tTest-mae:0.19549\n",
      "[274]\tTest-mae:0.19549\n",
      "[275]\tTest-mae:0.19548\n",
      "[276]\tTest-mae:0.19545\n",
      "[277]\tTest-mae:0.19543\n",
      "[278]\tTest-mae:0.19541\n",
      "[279]\tTest-mae:0.19540\n",
      "[280]\tTest-mae:0.19537\n",
      "[281]\tTest-mae:0.19531\n",
      "[282]\tTest-mae:0.19528\n",
      "[283]\tTest-mae:0.19527\n",
      "[284]\tTest-mae:0.19526\n",
      "[285]\tTest-mae:0.19526\n",
      "[286]\tTest-mae:0.19526\n",
      "[287]\tTest-mae:0.19523\n",
      "[288]\tTest-mae:0.19518\n",
      "[289]\tTest-mae:0.19515\n",
      "[290]\tTest-mae:0.19510\n",
      "[291]\tTest-mae:0.19508\n",
      "[292]\tTest-mae:0.19504\n",
      "[293]\tTest-mae:0.19498\n",
      "[294]\tTest-mae:0.19494\n",
      "[295]\tTest-mae:0.19491\n",
      "[296]\tTest-mae:0.19490\n",
      "[297]\tTest-mae:0.19489\n",
      "[298]\tTest-mae:0.19487\n",
      "[299]\tTest-mae:0.19484\n",
      "[300]\tTest-mae:0.19480\n",
      "[301]\tTest-mae:0.19479\n",
      "[302]\tTest-mae:0.19477\n",
      "[303]\tTest-mae:0.19474\n",
      "[304]\tTest-mae:0.19469\n",
      "[305]\tTest-mae:0.19469\n",
      "[306]\tTest-mae:0.19467\n",
      "[307]\tTest-mae:0.19466\n",
      "[308]\tTest-mae:0.19465\n",
      "[309]\tTest-mae:0.19462\n",
      "[310]\tTest-mae:0.19460\n",
      "[311]\tTest-mae:0.19460\n",
      "[312]\tTest-mae:0.19459\n",
      "[313]\tTest-mae:0.19459\n",
      "[314]\tTest-mae:0.19455\n",
      "[315]\tTest-mae:0.19455\n",
      "[316]\tTest-mae:0.19453\n",
      "[317]\tTest-mae:0.19450\n",
      "[318]\tTest-mae:0.19446\n",
      "[319]\tTest-mae:0.19444\n",
      "[320]\tTest-mae:0.19441\n",
      "[321]\tTest-mae:0.19438\n",
      "[322]\tTest-mae:0.19433\n",
      "[323]\tTest-mae:0.19432\n",
      "[324]\tTest-mae:0.19430\n",
      "[325]\tTest-mae:0.19428\n",
      "[326]\tTest-mae:0.19425\n",
      "[327]\tTest-mae:0.19420\n",
      "[328]\tTest-mae:0.19418\n",
      "[329]\tTest-mae:0.19413\n",
      "[330]\tTest-mae:0.19411\n",
      "[331]\tTest-mae:0.19411\n",
      "[332]\tTest-mae:0.19410\n",
      "[333]\tTest-mae:0.19409\n",
      "[334]\tTest-mae:0.19408\n",
      "[335]\tTest-mae:0.19405\n",
      "[336]\tTest-mae:0.19402\n",
      "[337]\tTest-mae:0.19399\n",
      "[338]\tTest-mae:0.19397\n",
      "[339]\tTest-mae:0.19393\n",
      "[340]\tTest-mae:0.19392\n",
      "[341]\tTest-mae:0.19390\n",
      "[342]\tTest-mae:0.19388\n",
      "[343]\tTest-mae:0.19387\n",
      "[344]\tTest-mae:0.19384\n",
      "[345]\tTest-mae:0.19384\n",
      "[346]\tTest-mae:0.19385\n",
      "[347]\tTest-mae:0.19381\n",
      "[348]\tTest-mae:0.19378\n",
      "[349]\tTest-mae:0.19375\n",
      "[350]\tTest-mae:0.19370\n",
      "[351]\tTest-mae:0.19369\n",
      "[352]\tTest-mae:0.19369\n",
      "[353]\tTest-mae:0.19368\n",
      "[354]\tTest-mae:0.19367\n",
      "[355]\tTest-mae:0.19367\n",
      "[356]\tTest-mae:0.19363\n",
      "[357]\tTest-mae:0.19360\n",
      "[358]\tTest-mae:0.19356\n",
      "[359]\tTest-mae:0.19354\n",
      "[360]\tTest-mae:0.19351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[361]\tTest-mae:0.19351\n",
      "[362]\tTest-mae:0.19346\n",
      "[363]\tTest-mae:0.19344\n",
      "[364]\tTest-mae:0.19342\n",
      "[365]\tTest-mae:0.19341\n",
      "[366]\tTest-mae:0.19339\n",
      "[367]\tTest-mae:0.19339\n",
      "[368]\tTest-mae:0.19340\n",
      "[369]\tTest-mae:0.19337\n",
      "[370]\tTest-mae:0.19335\n",
      "[371]\tTest-mae:0.19332\n",
      "[372]\tTest-mae:0.19326\n",
      "[373]\tTest-mae:0.19325\n",
      "[374]\tTest-mae:0.19320\n",
      "[375]\tTest-mae:0.19317\n",
      "[376]\tTest-mae:0.19315\n",
      "[377]\tTest-mae:0.19315\n",
      "[378]\tTest-mae:0.19315\n",
      "[379]\tTest-mae:0.19314\n",
      "[380]\tTest-mae:0.19314\n",
      "[381]\tTest-mae:0.19313\n",
      "[382]\tTest-mae:0.19309\n",
      "[383]\tTest-mae:0.19309\n",
      "[384]\tTest-mae:0.19309\n",
      "[385]\tTest-mae:0.19309\n",
      "[386]\tTest-mae:0.19306\n",
      "[387]\tTest-mae:0.19305\n",
      "[388]\tTest-mae:0.19302\n",
      "[389]\tTest-mae:0.19302\n",
      "[390]\tTest-mae:0.19301\n",
      "[391]\tTest-mae:0.19301\n",
      "[392]\tTest-mae:0.19299\n",
      "[393]\tTest-mae:0.19298\n",
      "[394]\tTest-mae:0.19298\n",
      "[395]\tTest-mae:0.19296\n",
      "[396]\tTest-mae:0.19294\n",
      "[397]\tTest-mae:0.19293\n",
      "[398]\tTest-mae:0.19290\n",
      "[399]\tTest-mae:0.19289\n",
      "[400]\tTest-mae:0.19289\n",
      "[401]\tTest-mae:0.19286\n",
      "[402]\tTest-mae:0.19286\n",
      "[403]\tTest-mae:0.19285\n",
      "[404]\tTest-mae:0.19283\n",
      "[405]\tTest-mae:0.19281\n",
      "[406]\tTest-mae:0.19279\n",
      "[407]\tTest-mae:0.19277\n",
      "[408]\tTest-mae:0.19276\n",
      "[409]\tTest-mae:0.19271\n",
      "[410]\tTest-mae:0.19269\n",
      "[411]\tTest-mae:0.19271\n",
      "[412]\tTest-mae:0.19269\n",
      "[413]\tTest-mae:0.19268\n",
      "[414]\tTest-mae:0.19266\n",
      "[415]\tTest-mae:0.19266\n",
      "[416]\tTest-mae:0.19266\n",
      "[417]\tTest-mae:0.19263\n",
      "[418]\tTest-mae:0.19261\n",
      "[419]\tTest-mae:0.19261\n",
      "[420]\tTest-mae:0.19259\n",
      "[421]\tTest-mae:0.19256\n",
      "[422]\tTest-mae:0.19255\n",
      "[423]\tTest-mae:0.19254\n",
      "[424]\tTest-mae:0.19251\n",
      "[425]\tTest-mae:0.19251\n",
      "[426]\tTest-mae:0.19251\n",
      "[427]\tTest-mae:0.19251\n",
      "[428]\tTest-mae:0.19251\n",
      "[429]\tTest-mae:0.19250\n",
      "[430]\tTest-mae:0.19249\n",
      "[431]\tTest-mae:0.19248\n",
      "[432]\tTest-mae:0.19246\n",
      "[433]\tTest-mae:0.19246\n",
      "[434]\tTest-mae:0.19243\n",
      "[435]\tTest-mae:0.19243\n",
      "[436]\tTest-mae:0.19240\n",
      "[437]\tTest-mae:0.19239\n",
      "[438]\tTest-mae:0.19237\n",
      "[439]\tTest-mae:0.19237\n",
      "[440]\tTest-mae:0.19237\n",
      "[441]\tTest-mae:0.19236\n",
      "[442]\tTest-mae:0.19233\n",
      "[443]\tTest-mae:0.19232\n",
      "[444]\tTest-mae:0.19229\n",
      "[445]\tTest-mae:0.19228\n",
      "[446]\tTest-mae:0.19228\n",
      "[447]\tTest-mae:0.19226\n",
      "[448]\tTest-mae:0.19227\n",
      "[449]\tTest-mae:0.19225\n",
      "[450]\tTest-mae:0.19225\n",
      "[451]\tTest-mae:0.19224\n",
      "[452]\tTest-mae:0.19224\n",
      "[453]\tTest-mae:0.19221\n",
      "[454]\tTest-mae:0.19217\n",
      "[455]\tTest-mae:0.19217\n",
      "[456]\tTest-mae:0.19216\n",
      "[457]\tTest-mae:0.19216\n",
      "[458]\tTest-mae:0.19216\n",
      "[459]\tTest-mae:0.19215\n",
      "[460]\tTest-mae:0.19213\n",
      "[461]\tTest-mae:0.19209\n",
      "[462]\tTest-mae:0.19209\n",
      "[463]\tTest-mae:0.19207\n",
      "[464]\tTest-mae:0.19206\n",
      "[465]\tTest-mae:0.19205\n",
      "[466]\tTest-mae:0.19202\n",
      "[467]\tTest-mae:0.19201\n",
      "[468]\tTest-mae:0.19202\n",
      "[469]\tTest-mae:0.19201\n",
      "[470]\tTest-mae:0.19199\n",
      "[471]\tTest-mae:0.19198\n",
      "[472]\tTest-mae:0.19197\n",
      "[473]\tTest-mae:0.19196\n",
      "[474]\tTest-mae:0.19194\n",
      "[475]\tTest-mae:0.19190\n",
      "[476]\tTest-mae:0.19185\n",
      "[477]\tTest-mae:0.19183\n",
      "[478]\tTest-mae:0.19183\n",
      "[479]\tTest-mae:0.19182\n",
      "[480]\tTest-mae:0.19180\n",
      "[481]\tTest-mae:0.19179\n",
      "[482]\tTest-mae:0.19178\n",
      "[483]\tTest-mae:0.19177\n",
      "[484]\tTest-mae:0.19173\n",
      "[485]\tTest-mae:0.19170\n",
      "[486]\tTest-mae:0.19168\n",
      "[487]\tTest-mae:0.19166\n",
      "[488]\tTest-mae:0.19165\n",
      "[489]\tTest-mae:0.19166\n",
      "[490]\tTest-mae:0.19162\n",
      "[491]\tTest-mae:0.19161\n",
      "[492]\tTest-mae:0.19161\n",
      "[493]\tTest-mae:0.19160\n",
      "[494]\tTest-mae:0.19159\n",
      "[495]\tTest-mae:0.19159\n",
      "[496]\tTest-mae:0.19160\n",
      "[497]\tTest-mae:0.19159\n",
      "[498]\tTest-mae:0.19156\n",
      "[499]\tTest-mae:0.19156\n",
      "[500]\tTest-mae:0.19157\n",
      "[501]\tTest-mae:0.19156\n",
      "[502]\tTest-mae:0.19154\n",
      "[503]\tTest-mae:0.19153\n",
      "[504]\tTest-mae:0.19152\n",
      "[505]\tTest-mae:0.19151\n",
      "[506]\tTest-mae:0.19148\n",
      "[507]\tTest-mae:0.19148\n",
      "[508]\tTest-mae:0.19147\n",
      "[509]\tTest-mae:0.19143\n",
      "[510]\tTest-mae:0.19141\n",
      "[511]\tTest-mae:0.19140\n",
      "[512]\tTest-mae:0.19140\n",
      "[513]\tTest-mae:0.19138\n",
      "[514]\tTest-mae:0.19138\n",
      "[515]\tTest-mae:0.19136\n",
      "[516]\tTest-mae:0.19136\n",
      "[517]\tTest-mae:0.19134\n",
      "[518]\tTest-mae:0.19131\n",
      "[519]\tTest-mae:0.19130\n",
      "[520]\tTest-mae:0.19130\n",
      "[521]\tTest-mae:0.19129\n",
      "[522]\tTest-mae:0.19128\n",
      "[523]\tTest-mae:0.19127\n",
      "[524]\tTest-mae:0.19126\n",
      "[525]\tTest-mae:0.19124\n",
      "[526]\tTest-mae:0.19123\n",
      "[527]\tTest-mae:0.19122\n",
      "[528]\tTest-mae:0.19122\n",
      "[529]\tTest-mae:0.19121\n",
      "[530]\tTest-mae:0.19119\n",
      "[531]\tTest-mae:0.19120\n",
      "[532]\tTest-mae:0.19120\n",
      "[533]\tTest-mae:0.19117\n",
      "[534]\tTest-mae:0.19115\n",
      "[535]\tTest-mae:0.19114\n",
      "[536]\tTest-mae:0.19113\n",
      "[537]\tTest-mae:0.19112\n",
      "[538]\tTest-mae:0.19109\n",
      "[539]\tTest-mae:0.19107\n",
      "[540]\tTest-mae:0.19106\n",
      "[541]\tTest-mae:0.19105\n",
      "[542]\tTest-mae:0.19103\n",
      "[543]\tTest-mae:0.19103\n",
      "[544]\tTest-mae:0.19101\n",
      "[545]\tTest-mae:0.19101\n",
      "[546]\tTest-mae:0.19101\n",
      "[547]\tTest-mae:0.19101\n",
      "[548]\tTest-mae:0.19101\n",
      "[549]\tTest-mae:0.19101\n",
      "[550]\tTest-mae:0.19101\n",
      "[551]\tTest-mae:0.19101\n",
      "[552]\tTest-mae:0.19098\n",
      "[553]\tTest-mae:0.19098\n",
      "[554]\tTest-mae:0.19096\n",
      "[555]\tTest-mae:0.19095\n",
      "[556]\tTest-mae:0.19096\n",
      "[557]\tTest-mae:0.19095\n",
      "[558]\tTest-mae:0.19095\n",
      "[559]\tTest-mae:0.19094\n",
      "[560]\tTest-mae:0.19091\n",
      "[561]\tTest-mae:0.19089\n",
      "[562]\tTest-mae:0.19090\n",
      "[563]\tTest-mae:0.19090\n",
      "[564]\tTest-mae:0.19088\n",
      "[565]\tTest-mae:0.19087\n",
      "[566]\tTest-mae:0.19087\n",
      "[567]\tTest-mae:0.19085\n",
      "[568]\tTest-mae:0.19085\n",
      "[569]\tTest-mae:0.19083\n",
      "[570]\tTest-mae:0.19079\n",
      "[571]\tTest-mae:0.19078\n",
      "[572]\tTest-mae:0.19076\n",
      "[573]\tTest-mae:0.19077\n",
      "[574]\tTest-mae:0.19074\n",
      "[575]\tTest-mae:0.19072\n",
      "[576]\tTest-mae:0.19071\n",
      "[577]\tTest-mae:0.19072\n",
      "[578]\tTest-mae:0.19073\n",
      "[579]\tTest-mae:0.19073\n",
      "[580]\tTest-mae:0.19073\n",
      "[581]\tTest-mae:0.19071\n",
      "[582]\tTest-mae:0.19069\n",
      "[583]\tTest-mae:0.19070\n",
      "[584]\tTest-mae:0.19067\n",
      "[585]\tTest-mae:0.19068\n",
      "[586]\tTest-mae:0.19068\n",
      "[587]\tTest-mae:0.19067\n",
      "[588]\tTest-mae:0.19066\n",
      "[589]\tTest-mae:0.19068\n",
      "[590]\tTest-mae:0.19065\n",
      "[591]\tTest-mae:0.19064\n",
      "[592]\tTest-mae:0.19062\n",
      "[593]\tTest-mae:0.19062\n",
      "[594]\tTest-mae:0.19061\n",
      "[595]\tTest-mae:0.19060\n",
      "[596]\tTest-mae:0.19058\n",
      "[597]\tTest-mae:0.19057\n",
      "[598]\tTest-mae:0.19056\n",
      "[599]\tTest-mae:0.19057\n",
      "[600]\tTest-mae:0.19057\n",
      "[601]\tTest-mae:0.19055\n",
      "[602]\tTest-mae:0.19054\n",
      "[603]\tTest-mae:0.19052\n",
      "[604]\tTest-mae:0.19050\n",
      "[605]\tTest-mae:0.19050\n",
      "[606]\tTest-mae:0.19048\n",
      "[607]\tTest-mae:0.19046\n",
      "[608]\tTest-mae:0.19045\n",
      "[609]\tTest-mae:0.19044\n",
      "[610]\tTest-mae:0.19043\n",
      "[611]\tTest-mae:0.19043\n",
      "[612]\tTest-mae:0.19043\n",
      "[613]\tTest-mae:0.19042\n",
      "[614]\tTest-mae:0.19041\n",
      "[615]\tTest-mae:0.19040\n",
      "[616]\tTest-mae:0.19039\n",
      "[617]\tTest-mae:0.19037\n",
      "[618]\tTest-mae:0.19036\n",
      "[619]\tTest-mae:0.19036\n",
      "[620]\tTest-mae:0.19035\n",
      "[621]\tTest-mae:0.19034\n",
      "[622]\tTest-mae:0.19034\n",
      "[623]\tTest-mae:0.19032\n",
      "[624]\tTest-mae:0.19033\n",
      "[625]\tTest-mae:0.19032\n",
      "[626]\tTest-mae:0.19030\n",
      "[627]\tTest-mae:0.19028\n",
      "[628]\tTest-mae:0.19025\n",
      "[629]\tTest-mae:0.19025\n",
      "[630]\tTest-mae:0.19025\n",
      "[631]\tTest-mae:0.19024\n",
      "[632]\tTest-mae:0.19023\n",
      "[633]\tTest-mae:0.19022\n",
      "[634]\tTest-mae:0.19022\n",
      "[635]\tTest-mae:0.19021\n",
      "[636]\tTest-mae:0.19021\n",
      "[637]\tTest-mae:0.19020\n",
      "[638]\tTest-mae:0.19018\n",
      "[639]\tTest-mae:0.19017\n",
      "[640]\tTest-mae:0.19015\n",
      "[641]\tTest-mae:0.19015\n",
      "[642]\tTest-mae:0.19014\n",
      "[643]\tTest-mae:0.19013\n",
      "[644]\tTest-mae:0.19014\n",
      "[645]\tTest-mae:0.19012\n",
      "[646]\tTest-mae:0.19013\n",
      "[647]\tTest-mae:0.19013\n",
      "[648]\tTest-mae:0.19011\n",
      "[649]\tTest-mae:0.19010\n",
      "[650]\tTest-mae:0.19010\n",
      "[651]\tTest-mae:0.19008\n",
      "[652]\tTest-mae:0.19008\n",
      "[653]\tTest-mae:0.19008\n",
      "[654]\tTest-mae:0.19008\n",
      "[655]\tTest-mae:0.19007\n",
      "[656]\tTest-mae:0.19007\n",
      "[657]\tTest-mae:0.19006\n",
      "[658]\tTest-mae:0.19004\n",
      "[659]\tTest-mae:0.19004\n",
      "[660]\tTest-mae:0.19004\n",
      "[661]\tTest-mae:0.19003\n",
      "[662]\tTest-mae:0.19002\n",
      "[663]\tTest-mae:0.19002\n",
      "[664]\tTest-mae:0.19002\n",
      "[665]\tTest-mae:0.19002\n",
      "[666]\tTest-mae:0.19000\n",
      "[667]\tTest-mae:0.18999\n",
      "[668]\tTest-mae:0.19000\n",
      "[669]\tTest-mae:0.18999\n",
      "[670]\tTest-mae:0.18999\n",
      "[671]\tTest-mae:0.18998\n",
      "[672]\tTest-mae:0.18996\n",
      "[673]\tTest-mae:0.18996\n",
      "[674]\tTest-mae:0.18996\n",
      "[675]\tTest-mae:0.18996\n",
      "[676]\tTest-mae:0.18995\n",
      "[677]\tTest-mae:0.18995\n",
      "[678]\tTest-mae:0.18995\n",
      "[679]\tTest-mae:0.18995\n",
      "[680]\tTest-mae:0.18994\n",
      "[681]\tTest-mae:0.18993\n",
      "[682]\tTest-mae:0.18992\n",
      "[683]\tTest-mae:0.18990\n",
      "[684]\tTest-mae:0.18989\n",
      "[685]\tTest-mae:0.18988\n",
      "[686]\tTest-mae:0.18986\n",
      "[687]\tTest-mae:0.18986\n",
      "[688]\tTest-mae:0.18986\n",
      "[689]\tTest-mae:0.18986\n",
      "[690]\tTest-mae:0.18985\n",
      "[691]\tTest-mae:0.18985\n",
      "[692]\tTest-mae:0.18984\n",
      "[693]\tTest-mae:0.18982\n",
      "[694]\tTest-mae:0.18982\n",
      "[695]\tTest-mae:0.18980\n",
      "[696]\tTest-mae:0.18980\n",
      "[697]\tTest-mae:0.18980\n",
      "[698]\tTest-mae:0.18979\n",
      "[699]\tTest-mae:0.18979\n",
      "[700]\tTest-mae:0.18978\n",
      "[701]\tTest-mae:0.18978\n",
      "[702]\tTest-mae:0.18977\n",
      "[703]\tTest-mae:0.18976\n",
      "[704]\tTest-mae:0.18976\n",
      "[705]\tTest-mae:0.18975\n",
      "[706]\tTest-mae:0.18974\n",
      "[707]\tTest-mae:0.18974\n",
      "[708]\tTest-mae:0.18973\n",
      "[709]\tTest-mae:0.18973\n",
      "[710]\tTest-mae:0.18971\n",
      "[711]\tTest-mae:0.18971\n",
      "[712]\tTest-mae:0.18970\n",
      "[713]\tTest-mae:0.18968\n",
      "[714]\tTest-mae:0.18968\n",
      "[715]\tTest-mae:0.18967\n",
      "[716]\tTest-mae:0.18968\n",
      "[717]\tTest-mae:0.18969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718]\tTest-mae:0.18968\n",
      "[719]\tTest-mae:0.18968\n",
      "[720]\tTest-mae:0.18967\n",
      "[721]\tTest-mae:0.18967\n",
      "[722]\tTest-mae:0.18966\n",
      "[723]\tTest-mae:0.18965\n",
      "[724]\tTest-mae:0.18964\n",
      "[725]\tTest-mae:0.18965\n",
      "[726]\tTest-mae:0.18965\n",
      "[727]\tTest-mae:0.18965\n",
      "[728]\tTest-mae:0.18965\n",
      "[729]\tTest-mae:0.18965\n",
      "[730]\tTest-mae:0.18965\n",
      "[731]\tTest-mae:0.18965\n",
      "[732]\tTest-mae:0.18963\n",
      "[733]\tTest-mae:0.18962\n",
      "[734]\tTest-mae:0.18962\n",
      "[735]\tTest-mae:0.18961\n",
      "[736]\tTest-mae:0.18960\n",
      "[737]\tTest-mae:0.18958\n",
      "[738]\tTest-mae:0.18958\n",
      "[739]\tTest-mae:0.18958\n",
      "[740]\tTest-mae:0.18959\n",
      "[741]\tTest-mae:0.18957\n",
      "[742]\tTest-mae:0.18957\n",
      "[743]\tTest-mae:0.18956\n",
      "[744]\tTest-mae:0.18956\n",
      "[745]\tTest-mae:0.18956\n",
      "[746]\tTest-mae:0.18954\n",
      "[747]\tTest-mae:0.18953\n",
      "[748]\tTest-mae:0.18954\n",
      "[749]\tTest-mae:0.18953\n",
      "[750]\tTest-mae:0.18953\n",
      "[751]\tTest-mae:0.18954\n",
      "[752]\tTest-mae:0.18952\n",
      "[753]\tTest-mae:0.18951\n",
      "[754]\tTest-mae:0.18951\n",
      "[755]\tTest-mae:0.18951\n",
      "[756]\tTest-mae:0.18951\n",
      "[757]\tTest-mae:0.18951\n",
      "[758]\tTest-mae:0.18951\n",
      "[759]\tTest-mae:0.18950\n",
      "[760]\tTest-mae:0.18949\n",
      "[761]\tTest-mae:0.18949\n",
      "[762]\tTest-mae:0.18948\n",
      "[763]\tTest-mae:0.18947\n",
      "[764]\tTest-mae:0.18942\n",
      "[765]\tTest-mae:0.18942\n",
      "[766]\tTest-mae:0.18940\n",
      "[767]\tTest-mae:0.18940\n",
      "[768]\tTest-mae:0.18937\n",
      "[769]\tTest-mae:0.18937\n",
      "[770]\tTest-mae:0.18935\n",
      "[771]\tTest-mae:0.18935\n",
      "[772]\tTest-mae:0.18935\n",
      "[773]\tTest-mae:0.18934\n",
      "[774]\tTest-mae:0.18934\n",
      "[775]\tTest-mae:0.18932\n",
      "[776]\tTest-mae:0.18932\n",
      "[777]\tTest-mae:0.18933\n",
      "[778]\tTest-mae:0.18932\n",
      "[779]\tTest-mae:0.18932\n",
      "[780]\tTest-mae:0.18932\n",
      "[781]\tTest-mae:0.18933\n",
      "[782]\tTest-mae:0.18932\n",
      "[783]\tTest-mae:0.18931\n",
      "[784]\tTest-mae:0.18930\n",
      "[785]\tTest-mae:0.18928\n",
      "[786]\tTest-mae:0.18928\n",
      "[787]\tTest-mae:0.18929\n",
      "[788]\tTest-mae:0.18928\n",
      "[789]\tTest-mae:0.18927\n",
      "[790]\tTest-mae:0.18928\n",
      "[791]\tTest-mae:0.18926\n",
      "[792]\tTest-mae:0.18924\n",
      "[793]\tTest-mae:0.18925\n",
      "[794]\tTest-mae:0.18924\n",
      "[795]\tTest-mae:0.18923\n",
      "[796]\tTest-mae:0.18923\n",
      "[797]\tTest-mae:0.18921\n",
      "[798]\tTest-mae:0.18920\n",
      "[799]\tTest-mae:0.18920\n",
      "[800]\tTest-mae:0.18919\n",
      "[801]\tTest-mae:0.18919\n",
      "[802]\tTest-mae:0.18918\n",
      "[803]\tTest-mae:0.18919\n",
      "[804]\tTest-mae:0.18919\n",
      "[805]\tTest-mae:0.18919\n",
      "[806]\tTest-mae:0.18919\n",
      "[807]\tTest-mae:0.18918\n",
      "[808]\tTest-mae:0.18919\n",
      "[809]\tTest-mae:0.18918\n",
      "[810]\tTest-mae:0.18918\n",
      "[811]\tTest-mae:0.18918\n",
      "[812]\tTest-mae:0.18917\n",
      "[813]\tTest-mae:0.18914\n",
      "[814]\tTest-mae:0.18914\n",
      "[815]\tTest-mae:0.18914\n",
      "[816]\tTest-mae:0.18914\n",
      "[817]\tTest-mae:0.18913\n",
      "[818]\tTest-mae:0.18914\n",
      "[819]\tTest-mae:0.18912\n",
      "[820]\tTest-mae:0.18912\n",
      "[821]\tTest-mae:0.18912\n",
      "[822]\tTest-mae:0.18912\n",
      "[823]\tTest-mae:0.18910\n",
      "[824]\tTest-mae:0.18911\n",
      "[825]\tTest-mae:0.18910\n",
      "[826]\tTest-mae:0.18910\n",
      "[827]\tTest-mae:0.18911\n",
      "[828]\tTest-mae:0.18911\n",
      "[829]\tTest-mae:0.18911\n",
      "[830]\tTest-mae:0.18911\n",
      "[831]\tTest-mae:0.18910\n",
      "[832]\tTest-mae:0.18910\n",
      "[833]\tTest-mae:0.18910\n",
      "[834]\tTest-mae:0.18910\n",
      "[835]\tTest-mae:0.18912\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:12.34912\n",
      "[1]\tTest-mae:11.73199\n",
      "[2]\tTest-mae:11.14561\n",
      "[3]\tTest-mae:10.58857\n",
      "[4]\tTest-mae:10.05947\n",
      "[5]\tTest-mae:9.55672\n",
      "[6]\tTest-mae:9.07917\n",
      "[7]\tTest-mae:8.62550\n",
      "[8]\tTest-mae:8.19450\n",
      "[9]\tTest-mae:7.78502\n",
      "[10]\tTest-mae:7.39600\n",
      "[11]\tTest-mae:7.02643\n",
      "[12]\tTest-mae:6.67531\n",
      "[13]\tTest-mae:6.34183\n",
      "[14]\tTest-mae:6.02498\n",
      "[15]\tTest-mae:5.72384\n",
      "[16]\tTest-mae:5.43780\n",
      "[17]\tTest-mae:5.16606\n",
      "[18]\tTest-mae:4.90800\n",
      "[19]\tTest-mae:4.66278\n",
      "[20]\tTest-mae:4.42979\n",
      "[21]\tTest-mae:4.20846\n",
      "[22]\tTest-mae:3.99813\n",
      "[23]\tTest-mae:3.79838\n",
      "[24]\tTest-mae:3.60855\n",
      "[25]\tTest-mae:3.42815\n",
      "[26]\tTest-mae:3.25703\n",
      "[27]\tTest-mae:3.09434\n",
      "[28]\tTest-mae:2.93977\n",
      "[29]\tTest-mae:2.79286\n",
      "[30]\tTest-mae:2.65345\n",
      "[31]\tTest-mae:2.52101\n",
      "[32]\tTest-mae:2.39507\n",
      "[33]\tTest-mae:2.27558\n",
      "[34]\tTest-mae:2.16199\n",
      "[35]\tTest-mae:2.05416\n",
      "[36]\tTest-mae:1.95175\n",
      "[37]\tTest-mae:1.85456\n",
      "[38]\tTest-mae:1.76229\n",
      "[39]\tTest-mae:1.67478\n",
      "[40]\tTest-mae:1.59170\n",
      "[41]\tTest-mae:1.51293\n",
      "[42]\tTest-mae:1.43828\n",
      "[43]\tTest-mae:1.36731\n",
      "[44]\tTest-mae:1.30011\n",
      "[45]\tTest-mae:1.23631\n",
      "[46]\tTest-mae:1.17572\n",
      "[47]\tTest-mae:1.11830\n",
      "[48]\tTest-mae:1.06398\n",
      "[49]\tTest-mae:1.01254\n",
      "[50]\tTest-mae:0.96391\n",
      "[51]\tTest-mae:0.91787\n",
      "[52]\tTest-mae:0.87447\n",
      "[53]\tTest-mae:0.83340\n",
      "[54]\tTest-mae:0.79460\n",
      "[55]\tTest-mae:0.75782\n",
      "[56]\tTest-mae:0.72321\n",
      "[57]\tTest-mae:0.69033\n",
      "[58]\tTest-mae:0.65948\n",
      "[59]\tTest-mae:0.63043\n",
      "[60]\tTest-mae:0.60295\n",
      "[61]\tTest-mae:0.57714\n",
      "[62]\tTest-mae:0.55273\n",
      "[63]\tTest-mae:0.52965\n",
      "[64]\tTest-mae:0.50802\n",
      "[65]\tTest-mae:0.48763\n",
      "[66]\tTest-mae:0.46844\n",
      "[67]\tTest-mae:0.45049\n",
      "[68]\tTest-mae:0.43366\n",
      "[69]\tTest-mae:0.41792\n",
      "[70]\tTest-mae:0.40303\n",
      "[71]\tTest-mae:0.38917\n",
      "[72]\tTest-mae:0.37606\n",
      "[73]\tTest-mae:0.36394\n",
      "[74]\tTest-mae:0.35248\n",
      "[75]\tTest-mae:0.34180\n",
      "[76]\tTest-mae:0.33178\n",
      "[77]\tTest-mae:0.32233\n",
      "[78]\tTest-mae:0.31361\n",
      "[79]\tTest-mae:0.30544\n",
      "[80]\tTest-mae:0.29798\n",
      "[81]\tTest-mae:0.29093\n",
      "[82]\tTest-mae:0.28443\n",
      "[83]\tTest-mae:0.27833\n",
      "[84]\tTest-mae:0.27276\n",
      "[85]\tTest-mae:0.26743\n",
      "[86]\tTest-mae:0.26265\n",
      "[87]\tTest-mae:0.25820\n",
      "[88]\tTest-mae:0.25411\n",
      "[89]\tTest-mae:0.25026\n",
      "[90]\tTest-mae:0.24674\n",
      "[91]\tTest-mae:0.24349\n",
      "[92]\tTest-mae:0.24052\n",
      "[93]\tTest-mae:0.23778\n",
      "[94]\tTest-mae:0.23517\n",
      "[95]\tTest-mae:0.23282\n",
      "[96]\tTest-mae:0.23063\n",
      "[97]\tTest-mae:0.22860\n",
      "[98]\tTest-mae:0.22668\n",
      "[99]\tTest-mae:0.22500\n",
      "[100]\tTest-mae:0.22340\n",
      "[101]\tTest-mae:0.22199\n",
      "[102]\tTest-mae:0.22072\n",
      "[103]\tTest-mae:0.21944\n",
      "[104]\tTest-mae:0.21827\n",
      "[105]\tTest-mae:0.21716\n",
      "[106]\tTest-mae:0.21619\n",
      "[107]\tTest-mae:0.21530\n",
      "[108]\tTest-mae:0.21439\n",
      "[109]\tTest-mae:0.21363\n",
      "[110]\tTest-mae:0.21289\n",
      "[111]\tTest-mae:0.21219\n",
      "[112]\tTest-mae:0.21155\n",
      "[113]\tTest-mae:0.21102\n",
      "[114]\tTest-mae:0.21046\n",
      "[115]\tTest-mae:0.20989\n",
      "[116]\tTest-mae:0.20936\n",
      "[117]\tTest-mae:0.20894\n",
      "[118]\tTest-mae:0.20855\n",
      "[119]\tTest-mae:0.20809\n",
      "[120]\tTest-mae:0.20778\n",
      "[121]\tTest-mae:0.20740\n",
      "[122]\tTest-mae:0.20702\n",
      "[123]\tTest-mae:0.20675\n",
      "[124]\tTest-mae:0.20643\n",
      "[125]\tTest-mae:0.20617\n",
      "[126]\tTest-mae:0.20588\n",
      "[127]\tTest-mae:0.20567\n",
      "[128]\tTest-mae:0.20542\n",
      "[129]\tTest-mae:0.20516\n",
      "[130]\tTest-mae:0.20491\n",
      "[131]\tTest-mae:0.20463\n",
      "[132]\tTest-mae:0.20447\n",
      "[133]\tTest-mae:0.20425\n",
      "[134]\tTest-mae:0.20407\n",
      "[135]\tTest-mae:0.20390\n",
      "[136]\tTest-mae:0.20377\n",
      "[137]\tTest-mae:0.20364\n",
      "[138]\tTest-mae:0.20350\n",
      "[139]\tTest-mae:0.20328\n",
      "[140]\tTest-mae:0.20306\n",
      "[141]\tTest-mae:0.20285\n",
      "[142]\tTest-mae:0.20269\n",
      "[143]\tTest-mae:0.20258\n",
      "[144]\tTest-mae:0.20249\n",
      "[145]\tTest-mae:0.20240\n",
      "[146]\tTest-mae:0.20230\n",
      "[147]\tTest-mae:0.20222\n",
      "[148]\tTest-mae:0.20211\n",
      "[149]\tTest-mae:0.20204\n",
      "[150]\tTest-mae:0.20192\n",
      "[151]\tTest-mae:0.20183\n",
      "[152]\tTest-mae:0.20173\n",
      "[153]\tTest-mae:0.20159\n",
      "[154]\tTest-mae:0.20144\n",
      "[155]\tTest-mae:0.20131\n",
      "[156]\tTest-mae:0.20124\n",
      "[157]\tTest-mae:0.20117\n",
      "[158]\tTest-mae:0.20110\n",
      "[159]\tTest-mae:0.20096\n",
      "[160]\tTest-mae:0.20086\n",
      "[161]\tTest-mae:0.20082\n",
      "[162]\tTest-mae:0.20075\n",
      "[163]\tTest-mae:0.20067\n",
      "[164]\tTest-mae:0.20064\n",
      "[165]\tTest-mae:0.20054\n",
      "[166]\tTest-mae:0.20046\n",
      "[167]\tTest-mae:0.20036\n",
      "[168]\tTest-mae:0.20031\n",
      "[169]\tTest-mae:0.20020\n",
      "[170]\tTest-mae:0.20009\n",
      "[171]\tTest-mae:0.20000\n",
      "[172]\tTest-mae:0.19996\n",
      "[173]\tTest-mae:0.19994\n",
      "[174]\tTest-mae:0.19987\n",
      "[175]\tTest-mae:0.19982\n",
      "[176]\tTest-mae:0.19973\n",
      "[177]\tTest-mae:0.19964\n",
      "[178]\tTest-mae:0.19954\n",
      "[179]\tTest-mae:0.19942\n",
      "[180]\tTest-mae:0.19937\n",
      "[181]\tTest-mae:0.19931\n",
      "[182]\tTest-mae:0.19929\n",
      "[183]\tTest-mae:0.19925\n",
      "[184]\tTest-mae:0.19919\n",
      "[185]\tTest-mae:0.19913\n",
      "[186]\tTest-mae:0.19912\n",
      "[187]\tTest-mae:0.19910\n",
      "[188]\tTest-mae:0.19904\n",
      "[189]\tTest-mae:0.19895\n",
      "[190]\tTest-mae:0.19886\n",
      "[191]\tTest-mae:0.19876\n",
      "[192]\tTest-mae:0.19872\n",
      "[193]\tTest-mae:0.19868\n",
      "[194]\tTest-mae:0.19867\n",
      "[195]\tTest-mae:0.19856\n",
      "[196]\tTest-mae:0.19846\n",
      "[197]\tTest-mae:0.19842\n",
      "[198]\tTest-mae:0.19837\n",
      "[199]\tTest-mae:0.19830\n",
      "[200]\tTest-mae:0.19827\n",
      "[201]\tTest-mae:0.19824\n",
      "[202]\tTest-mae:0.19818\n",
      "[203]\tTest-mae:0.19813\n",
      "[204]\tTest-mae:0.19807\n",
      "[205]\tTest-mae:0.19804\n",
      "[206]\tTest-mae:0.19802\n",
      "[207]\tTest-mae:0.19801\n",
      "[208]\tTest-mae:0.19794\n",
      "[209]\tTest-mae:0.19788\n",
      "[210]\tTest-mae:0.19780\n",
      "[211]\tTest-mae:0.19776\n",
      "[212]\tTest-mae:0.19773\n",
      "[213]\tTest-mae:0.19769\n",
      "[214]\tTest-mae:0.19762\n",
      "[215]\tTest-mae:0.19756\n",
      "[216]\tTest-mae:0.19755\n",
      "[217]\tTest-mae:0.19752\n",
      "[218]\tTest-mae:0.19748\n",
      "[219]\tTest-mae:0.19739\n",
      "[220]\tTest-mae:0.19735\n",
      "[221]\tTest-mae:0.19732\n",
      "[222]\tTest-mae:0.19728\n",
      "[223]\tTest-mae:0.19728\n",
      "[224]\tTest-mae:0.19726\n",
      "[225]\tTest-mae:0.19722\n",
      "[226]\tTest-mae:0.19716\n",
      "[227]\tTest-mae:0.19714\n",
      "[228]\tTest-mae:0.19707\n",
      "[229]\tTest-mae:0.19705\n",
      "[230]\tTest-mae:0.19701\n",
      "[231]\tTest-mae:0.19699\n",
      "[232]\tTest-mae:0.19694\n",
      "[233]\tTest-mae:0.19691\n",
      "[234]\tTest-mae:0.19689\n",
      "[235]\tTest-mae:0.19680\n",
      "[236]\tTest-mae:0.19676\n",
      "[237]\tTest-mae:0.19673\n",
      "[238]\tTest-mae:0.19671\n",
      "[239]\tTest-mae:0.19670\n",
      "[240]\tTest-mae:0.19664\n",
      "[241]\tTest-mae:0.19663\n",
      "[242]\tTest-mae:0.19654\n",
      "[243]\tTest-mae:0.19650\n",
      "[244]\tTest-mae:0.19645\n",
      "[245]\tTest-mae:0.19641\n",
      "[246]\tTest-mae:0.19635\n",
      "[247]\tTest-mae:0.19634\n",
      "[248]\tTest-mae:0.19632\n",
      "[249]\tTest-mae:0.19630\n",
      "[250]\tTest-mae:0.19626\n",
      "[251]\tTest-mae:0.19624\n",
      "[252]\tTest-mae:0.19620\n",
      "[253]\tTest-mae:0.19616\n",
      "[254]\tTest-mae:0.19615\n",
      "[255]\tTest-mae:0.19612\n",
      "[256]\tTest-mae:0.19609\n",
      "[257]\tTest-mae:0.19607\n",
      "[258]\tTest-mae:0.19602\n",
      "[259]\tTest-mae:0.19595\n",
      "[260]\tTest-mae:0.19593\n",
      "[261]\tTest-mae:0.19588\n",
      "[262]\tTest-mae:0.19586\n",
      "[263]\tTest-mae:0.19584\n",
      "[264]\tTest-mae:0.19579\n",
      "[265]\tTest-mae:0.19577\n",
      "[266]\tTest-mae:0.19572\n",
      "[267]\tTest-mae:0.19572\n",
      "[268]\tTest-mae:0.19570\n",
      "[269]\tTest-mae:0.19564\n",
      "[270]\tTest-mae:0.19556\n",
      "[271]\tTest-mae:0.19554\n",
      "[272]\tTest-mae:0.19552\n",
      "[273]\tTest-mae:0.19549\n",
      "[274]\tTest-mae:0.19549\n",
      "[275]\tTest-mae:0.19548\n",
      "[276]\tTest-mae:0.19545\n",
      "[277]\tTest-mae:0.19543\n",
      "[278]\tTest-mae:0.19541\n",
      "[279]\tTest-mae:0.19540\n",
      "[280]\tTest-mae:0.19537\n",
      "[281]\tTest-mae:0.19531\n",
      "[282]\tTest-mae:0.19528\n",
      "[283]\tTest-mae:0.19527\n",
      "[284]\tTest-mae:0.19526\n",
      "[285]\tTest-mae:0.19526\n",
      "[286]\tTest-mae:0.19526\n",
      "[287]\tTest-mae:0.19523\n",
      "[288]\tTest-mae:0.19518\n",
      "[289]\tTest-mae:0.19515\n",
      "[290]\tTest-mae:0.19510\n",
      "[291]\tTest-mae:0.19508\n",
      "[292]\tTest-mae:0.19504\n",
      "[293]\tTest-mae:0.19498\n",
      "[294]\tTest-mae:0.19494\n",
      "[295]\tTest-mae:0.19491\n",
      "[296]\tTest-mae:0.19490\n",
      "[297]\tTest-mae:0.19489\n",
      "[298]\tTest-mae:0.19487\n",
      "[299]\tTest-mae:0.19484\n",
      "[300]\tTest-mae:0.19480\n",
      "[301]\tTest-mae:0.19479\n",
      "[302]\tTest-mae:0.19477\n",
      "[303]\tTest-mae:0.19474\n",
      "[304]\tTest-mae:0.19469\n",
      "[305]\tTest-mae:0.19469\n",
      "[306]\tTest-mae:0.19467\n",
      "[307]\tTest-mae:0.19466\n",
      "[308]\tTest-mae:0.19465\n",
      "[309]\tTest-mae:0.19462\n",
      "[310]\tTest-mae:0.19460\n",
      "[311]\tTest-mae:0.19460\n",
      "[312]\tTest-mae:0.19459\n",
      "[313]\tTest-mae:0.19459\n",
      "[314]\tTest-mae:0.19455\n",
      "[315]\tTest-mae:0.19455\n",
      "[316]\tTest-mae:0.19453\n",
      "[317]\tTest-mae:0.19450\n",
      "[318]\tTest-mae:0.19446\n",
      "[319]\tTest-mae:0.19444\n",
      "[320]\tTest-mae:0.19441\n",
      "[321]\tTest-mae:0.19438\n",
      "[322]\tTest-mae:0.19433\n",
      "[323]\tTest-mae:0.19432\n",
      "[324]\tTest-mae:0.19430\n",
      "[325]\tTest-mae:0.19428\n",
      "[326]\tTest-mae:0.19425\n",
      "[327]\tTest-mae:0.19420\n",
      "[328]\tTest-mae:0.19418\n",
      "[329]\tTest-mae:0.19413\n",
      "[330]\tTest-mae:0.19411\n",
      "[331]\tTest-mae:0.19411\n",
      "[332]\tTest-mae:0.19410\n",
      "[333]\tTest-mae:0.19409\n",
      "[334]\tTest-mae:0.19408\n",
      "[335]\tTest-mae:0.19405\n",
      "[336]\tTest-mae:0.19402\n",
      "[337]\tTest-mae:0.19399\n",
      "[338]\tTest-mae:0.19397\n",
      "[339]\tTest-mae:0.19393\n",
      "[340]\tTest-mae:0.19392\n",
      "[341]\tTest-mae:0.19390\n",
      "[342]\tTest-mae:0.19388\n",
      "[343]\tTest-mae:0.19387\n",
      "[344]\tTest-mae:0.19384\n",
      "[345]\tTest-mae:0.19384\n",
      "[346]\tTest-mae:0.19385\n",
      "[347]\tTest-mae:0.19381\n",
      "[348]\tTest-mae:0.19378\n",
      "[349]\tTest-mae:0.19375\n",
      "[350]\tTest-mae:0.19370\n",
      "[351]\tTest-mae:0.19369\n",
      "[352]\tTest-mae:0.19369\n",
      "[353]\tTest-mae:0.19368\n",
      "[354]\tTest-mae:0.19367\n",
      "[355]\tTest-mae:0.19367\n",
      "[356]\tTest-mae:0.19363\n",
      "[357]\tTest-mae:0.19360\n",
      "[358]\tTest-mae:0.19356\n",
      "[359]\tTest-mae:0.19354\n",
      "[360]\tTest-mae:0.19351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[361]\tTest-mae:0.19351\n",
      "[362]\tTest-mae:0.19346\n",
      "[363]\tTest-mae:0.19344\n",
      "[364]\tTest-mae:0.19342\n",
      "[365]\tTest-mae:0.19341\n",
      "[366]\tTest-mae:0.19339\n",
      "[367]\tTest-mae:0.19339\n",
      "[368]\tTest-mae:0.19340\n",
      "[369]\tTest-mae:0.19337\n",
      "[370]\tTest-mae:0.19335\n",
      "[371]\tTest-mae:0.19332\n",
      "[372]\tTest-mae:0.19326\n",
      "[373]\tTest-mae:0.19325\n",
      "[374]\tTest-mae:0.19320\n",
      "[375]\tTest-mae:0.19317\n",
      "[376]\tTest-mae:0.19315\n",
      "[377]\tTest-mae:0.19315\n",
      "[378]\tTest-mae:0.19315\n",
      "[379]\tTest-mae:0.19314\n",
      "[380]\tTest-mae:0.19314\n",
      "[381]\tTest-mae:0.19313\n",
      "[382]\tTest-mae:0.19309\n",
      "[383]\tTest-mae:0.19309\n",
      "[384]\tTest-mae:0.19309\n",
      "[385]\tTest-mae:0.19309\n",
      "[386]\tTest-mae:0.19306\n",
      "[387]\tTest-mae:0.19305\n",
      "[388]\tTest-mae:0.19302\n",
      "[389]\tTest-mae:0.19302\n",
      "[390]\tTest-mae:0.19301\n",
      "[391]\tTest-mae:0.19301\n",
      "[392]\tTest-mae:0.19299\n",
      "[393]\tTest-mae:0.19298\n",
      "[394]\tTest-mae:0.19298\n",
      "[395]\tTest-mae:0.19296\n",
      "[396]\tTest-mae:0.19294\n",
      "[397]\tTest-mae:0.19293\n",
      "[398]\tTest-mae:0.19290\n",
      "[399]\tTest-mae:0.19289\n",
      "[400]\tTest-mae:0.19289\n",
      "[401]\tTest-mae:0.19286\n",
      "[402]\tTest-mae:0.19286\n",
      "[403]\tTest-mae:0.19285\n",
      "[404]\tTest-mae:0.19283\n",
      "[405]\tTest-mae:0.19281\n",
      "[406]\tTest-mae:0.19279\n",
      "[407]\tTest-mae:0.19277\n",
      "[408]\tTest-mae:0.19276\n",
      "[409]\tTest-mae:0.19271\n",
      "[410]\tTest-mae:0.19269\n",
      "[411]\tTest-mae:0.19271\n",
      "[412]\tTest-mae:0.19269\n",
      "[413]\tTest-mae:0.19268\n",
      "[414]\tTest-mae:0.19266\n",
      "[415]\tTest-mae:0.19266\n",
      "[416]\tTest-mae:0.19266\n",
      "[417]\tTest-mae:0.19263\n",
      "[418]\tTest-mae:0.19261\n",
      "[419]\tTest-mae:0.19261\n",
      "[420]\tTest-mae:0.19259\n",
      "[421]\tTest-mae:0.19256\n",
      "[422]\tTest-mae:0.19255\n",
      "[423]\tTest-mae:0.19254\n",
      "[424]\tTest-mae:0.19251\n",
      "[425]\tTest-mae:0.19251\n",
      "[426]\tTest-mae:0.19251\n",
      "[427]\tTest-mae:0.19251\n",
      "[428]\tTest-mae:0.19251\n",
      "[429]\tTest-mae:0.19250\n",
      "[430]\tTest-mae:0.19249\n",
      "[431]\tTest-mae:0.19248\n",
      "[432]\tTest-mae:0.19246\n",
      "[433]\tTest-mae:0.19246\n",
      "[434]\tTest-mae:0.19243\n",
      "[435]\tTest-mae:0.19243\n",
      "[436]\tTest-mae:0.19240\n",
      "[437]\tTest-mae:0.19239\n",
      "[438]\tTest-mae:0.19237\n",
      "[439]\tTest-mae:0.19237\n",
      "[440]\tTest-mae:0.19237\n",
      "[441]\tTest-mae:0.19236\n",
      "[442]\tTest-mae:0.19233\n",
      "[443]\tTest-mae:0.19232\n",
      "[444]\tTest-mae:0.19229\n",
      "[445]\tTest-mae:0.19228\n",
      "[446]\tTest-mae:0.19228\n",
      "[447]\tTest-mae:0.19226\n",
      "[448]\tTest-mae:0.19227\n",
      "[449]\tTest-mae:0.19225\n",
      "[450]\tTest-mae:0.19225\n",
      "[451]\tTest-mae:0.19224\n",
      "[452]\tTest-mae:0.19224\n",
      "[453]\tTest-mae:0.19221\n",
      "[454]\tTest-mae:0.19217\n",
      "[455]\tTest-mae:0.19217\n",
      "[456]\tTest-mae:0.19216\n",
      "[457]\tTest-mae:0.19216\n",
      "[458]\tTest-mae:0.19216\n",
      "[459]\tTest-mae:0.19215\n",
      "[460]\tTest-mae:0.19213\n",
      "[461]\tTest-mae:0.19209\n",
      "[462]\tTest-mae:0.19209\n",
      "[463]\tTest-mae:0.19207\n",
      "[464]\tTest-mae:0.19206\n",
      "[465]\tTest-mae:0.19205\n",
      "[466]\tTest-mae:0.19202\n",
      "[467]\tTest-mae:0.19201\n",
      "[468]\tTest-mae:0.19202\n",
      "[469]\tTest-mae:0.19201\n",
      "[470]\tTest-mae:0.19199\n",
      "[471]\tTest-mae:0.19198\n",
      "[472]\tTest-mae:0.19197\n",
      "[473]\tTest-mae:0.19196\n",
      "[474]\tTest-mae:0.19194\n",
      "[475]\tTest-mae:0.19190\n",
      "[476]\tTest-mae:0.19185\n",
      "[477]\tTest-mae:0.19183\n",
      "[478]\tTest-mae:0.19183\n",
      "[479]\tTest-mae:0.19182\n",
      "[480]\tTest-mae:0.19180\n",
      "[481]\tTest-mae:0.19179\n",
      "[482]\tTest-mae:0.19178\n",
      "[483]\tTest-mae:0.19177\n",
      "[484]\tTest-mae:0.19173\n",
      "[485]\tTest-mae:0.19170\n",
      "[486]\tTest-mae:0.19168\n",
      "[487]\tTest-mae:0.19166\n",
      "[488]\tTest-mae:0.19165\n",
      "[489]\tTest-mae:0.19166\n",
      "[490]\tTest-mae:0.19162\n",
      "[491]\tTest-mae:0.19161\n",
      "[492]\tTest-mae:0.19161\n",
      "[493]\tTest-mae:0.19160\n",
      "[494]\tTest-mae:0.19159\n",
      "[495]\tTest-mae:0.19159\n",
      "[496]\tTest-mae:0.19160\n",
      "[497]\tTest-mae:0.19159\n",
      "[498]\tTest-mae:0.19156\n",
      "[499]\tTest-mae:0.19156\n",
      "[500]\tTest-mae:0.19157\n",
      "[501]\tTest-mae:0.19156\n",
      "[502]\tTest-mae:0.19154\n",
      "[503]\tTest-mae:0.19153\n",
      "[504]\tTest-mae:0.19152\n",
      "[505]\tTest-mae:0.19151\n",
      "[506]\tTest-mae:0.19148\n",
      "[507]\tTest-mae:0.19148\n",
      "[508]\tTest-mae:0.19147\n",
      "[509]\tTest-mae:0.19143\n",
      "[510]\tTest-mae:0.19141\n",
      "[511]\tTest-mae:0.19140\n",
      "[512]\tTest-mae:0.19140\n",
      "[513]\tTest-mae:0.19138\n",
      "[514]\tTest-mae:0.19138\n",
      "[515]\tTest-mae:0.19136\n",
      "[516]\tTest-mae:0.19136\n",
      "[517]\tTest-mae:0.19134\n",
      "[518]\tTest-mae:0.19131\n",
      "[519]\tTest-mae:0.19130\n",
      "[520]\tTest-mae:0.19130\n",
      "[521]\tTest-mae:0.19129\n",
      "[522]\tTest-mae:0.19128\n",
      "[523]\tTest-mae:0.19127\n",
      "[524]\tTest-mae:0.19126\n",
      "[525]\tTest-mae:0.19124\n",
      "[526]\tTest-mae:0.19123\n",
      "[527]\tTest-mae:0.19122\n",
      "[528]\tTest-mae:0.19122\n",
      "[529]\tTest-mae:0.19121\n",
      "[530]\tTest-mae:0.19119\n",
      "[531]\tTest-mae:0.19120\n",
      "[532]\tTest-mae:0.19120\n",
      "[533]\tTest-mae:0.19117\n",
      "[534]\tTest-mae:0.19115\n",
      "[535]\tTest-mae:0.19114\n",
      "[536]\tTest-mae:0.19113\n",
      "[537]\tTest-mae:0.19112\n",
      "[538]\tTest-mae:0.19109\n",
      "[539]\tTest-mae:0.19107\n",
      "[540]\tTest-mae:0.19106\n",
      "[541]\tTest-mae:0.19105\n",
      "[542]\tTest-mae:0.19103\n",
      "[543]\tTest-mae:0.19103\n",
      "[544]\tTest-mae:0.19101\n",
      "[545]\tTest-mae:0.19101\n",
      "[546]\tTest-mae:0.19101\n",
      "[547]\tTest-mae:0.19101\n",
      "[548]\tTest-mae:0.19101\n",
      "[549]\tTest-mae:0.19101\n",
      "[550]\tTest-mae:0.19101\n",
      "[551]\tTest-mae:0.19101\n",
      "[552]\tTest-mae:0.19098\n",
      "[553]\tTest-mae:0.19098\n",
      "[554]\tTest-mae:0.19096\n",
      "[555]\tTest-mae:0.19095\n",
      "[556]\tTest-mae:0.19096\n",
      "[557]\tTest-mae:0.19095\n",
      "[558]\tTest-mae:0.19095\n",
      "[559]\tTest-mae:0.19094\n",
      "[560]\tTest-mae:0.19091\n",
      "[561]\tTest-mae:0.19089\n",
      "[562]\tTest-mae:0.19090\n",
      "[563]\tTest-mae:0.19090\n",
      "[564]\tTest-mae:0.19088\n",
      "[565]\tTest-mae:0.19087\n",
      "[566]\tTest-mae:0.19087\n",
      "[567]\tTest-mae:0.19085\n",
      "[568]\tTest-mae:0.19085\n",
      "[569]\tTest-mae:0.19083\n",
      "[570]\tTest-mae:0.19079\n",
      "[571]\tTest-mae:0.19078\n",
      "[572]\tTest-mae:0.19076\n",
      "[573]\tTest-mae:0.19077\n",
      "[574]\tTest-mae:0.19074\n",
      "[575]\tTest-mae:0.19072\n",
      "[576]\tTest-mae:0.19071\n",
      "[577]\tTest-mae:0.19072\n",
      "[578]\tTest-mae:0.19073\n",
      "[579]\tTest-mae:0.19073\n",
      "[580]\tTest-mae:0.19073\n",
      "[581]\tTest-mae:0.19071\n",
      "[582]\tTest-mae:0.19069\n",
      "[583]\tTest-mae:0.19070\n",
      "[584]\tTest-mae:0.19067\n",
      "[585]\tTest-mae:0.19068\n",
      "[586]\tTest-mae:0.19068\n",
      "[587]\tTest-mae:0.19067\n",
      "[588]\tTest-mae:0.19066\n",
      "[589]\tTest-mae:0.19068\n",
      "[590]\tTest-mae:0.19065\n",
      "[591]\tTest-mae:0.19064\n",
      "[592]\tTest-mae:0.19062\n",
      "[593]\tTest-mae:0.19062\n",
      "[594]\tTest-mae:0.19061\n",
      "[595]\tTest-mae:0.19060\n",
      "[596]\tTest-mae:0.19058\n",
      "[597]\tTest-mae:0.19057\n",
      "[598]\tTest-mae:0.19056\n",
      "[599]\tTest-mae:0.19057\n",
      "[600]\tTest-mae:0.19057\n",
      "[601]\tTest-mae:0.19055\n",
      "[602]\tTest-mae:0.19054\n",
      "[603]\tTest-mae:0.19052\n",
      "[604]\tTest-mae:0.19050\n",
      "[605]\tTest-mae:0.19050\n",
      "[606]\tTest-mae:0.19048\n",
      "[607]\tTest-mae:0.19046\n",
      "[608]\tTest-mae:0.19045\n",
      "[609]\tTest-mae:0.19044\n",
      "[610]\tTest-mae:0.19043\n",
      "[611]\tTest-mae:0.19043\n",
      "[612]\tTest-mae:0.19043\n",
      "[613]\tTest-mae:0.19042\n",
      "[614]\tTest-mae:0.19041\n",
      "[615]\tTest-mae:0.19040\n",
      "[616]\tTest-mae:0.19039\n",
      "[617]\tTest-mae:0.19037\n",
      "[618]\tTest-mae:0.19036\n",
      "[619]\tTest-mae:0.19036\n",
      "[620]\tTest-mae:0.19035\n",
      "[621]\tTest-mae:0.19034\n",
      "[622]\tTest-mae:0.19034\n",
      "[623]\tTest-mae:0.19032\n",
      "[624]\tTest-mae:0.19033\n",
      "[625]\tTest-mae:0.19032\n",
      "[626]\tTest-mae:0.19030\n",
      "[627]\tTest-mae:0.19028\n",
      "[628]\tTest-mae:0.19025\n",
      "[629]\tTest-mae:0.19025\n",
      "[630]\tTest-mae:0.19025\n",
      "[631]\tTest-mae:0.19024\n",
      "[632]\tTest-mae:0.19023\n",
      "[633]\tTest-mae:0.19022\n",
      "[634]\tTest-mae:0.19022\n",
      "[635]\tTest-mae:0.19021\n",
      "[636]\tTest-mae:0.19021\n",
      "[637]\tTest-mae:0.19020\n",
      "[638]\tTest-mae:0.19018\n",
      "[639]\tTest-mae:0.19017\n",
      "[640]\tTest-mae:0.19015\n",
      "[641]\tTest-mae:0.19015\n",
      "[642]\tTest-mae:0.19014\n",
      "[643]\tTest-mae:0.19013\n",
      "[644]\tTest-mae:0.19014\n",
      "[645]\tTest-mae:0.19012\n",
      "[646]\tTest-mae:0.19013\n",
      "[647]\tTest-mae:0.19013\n",
      "[648]\tTest-mae:0.19011\n",
      "[649]\tTest-mae:0.19010\n",
      "[650]\tTest-mae:0.19010\n",
      "[651]\tTest-mae:0.19008\n",
      "[652]\tTest-mae:0.19008\n",
      "[653]\tTest-mae:0.19008\n",
      "[654]\tTest-mae:0.19008\n",
      "[655]\tTest-mae:0.19007\n",
      "[656]\tTest-mae:0.19007\n",
      "[657]\tTest-mae:0.19006\n",
      "[658]\tTest-mae:0.19004\n",
      "[659]\tTest-mae:0.19004\n",
      "[660]\tTest-mae:0.19004\n",
      "[661]\tTest-mae:0.19003\n",
      "[662]\tTest-mae:0.19002\n",
      "[663]\tTest-mae:0.19002\n",
      "[664]\tTest-mae:0.19002\n",
      "[665]\tTest-mae:0.19002\n",
      "[666]\tTest-mae:0.19000\n",
      "[667]\tTest-mae:0.18999\n",
      "[668]\tTest-mae:0.19000\n",
      "[669]\tTest-mae:0.18999\n",
      "[670]\tTest-mae:0.18999\n",
      "[671]\tTest-mae:0.18998\n",
      "[672]\tTest-mae:0.18996\n",
      "[673]\tTest-mae:0.18996\n",
      "[674]\tTest-mae:0.18996\n",
      "[675]\tTest-mae:0.18996\n",
      "[676]\tTest-mae:0.18995\n",
      "[677]\tTest-mae:0.18995\n",
      "[678]\tTest-mae:0.18995\n",
      "[679]\tTest-mae:0.18995\n",
      "[680]\tTest-mae:0.18994\n",
      "[681]\tTest-mae:0.18993\n",
      "[682]\tTest-mae:0.18992\n",
      "[683]\tTest-mae:0.18990\n",
      "[684]\tTest-mae:0.18989\n",
      "[685]\tTest-mae:0.18988\n",
      "[686]\tTest-mae:0.18986\n",
      "[687]\tTest-mae:0.18986\n",
      "[688]\tTest-mae:0.18986\n",
      "[689]\tTest-mae:0.18986\n",
      "[690]\tTest-mae:0.18985\n",
      "[691]\tTest-mae:0.18985\n",
      "[692]\tTest-mae:0.18984\n",
      "[693]\tTest-mae:0.18982\n",
      "[694]\tTest-mae:0.18982\n",
      "[695]\tTest-mae:0.18980\n",
      "[696]\tTest-mae:0.18980\n",
      "[697]\tTest-mae:0.18980\n",
      "[698]\tTest-mae:0.18979\n",
      "[699]\tTest-mae:0.18979\n",
      "[700]\tTest-mae:0.18978\n",
      "[701]\tTest-mae:0.18978\n",
      "[702]\tTest-mae:0.18977\n",
      "[703]\tTest-mae:0.18976\n",
      "[704]\tTest-mae:0.18976\n",
      "[705]\tTest-mae:0.18975\n",
      "[706]\tTest-mae:0.18974\n",
      "[707]\tTest-mae:0.18974\n",
      "[708]\tTest-mae:0.18973\n",
      "[709]\tTest-mae:0.18973\n",
      "[710]\tTest-mae:0.18971\n",
      "[711]\tTest-mae:0.18971\n",
      "[712]\tTest-mae:0.18970\n",
      "[713]\tTest-mae:0.18968\n",
      "[714]\tTest-mae:0.18968\n",
      "[715]\tTest-mae:0.18967\n",
      "[716]\tTest-mae:0.18968\n",
      "[717]\tTest-mae:0.18969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718]\tTest-mae:0.18968\n",
      "[719]\tTest-mae:0.18968\n",
      "[720]\tTest-mae:0.18967\n",
      "[721]\tTest-mae:0.18967\n",
      "[722]\tTest-mae:0.18966\n",
      "[723]\tTest-mae:0.18965\n",
      "[724]\tTest-mae:0.18964\n",
      "[725]\tTest-mae:0.18965\n",
      "[726]\tTest-mae:0.18965\n",
      "[727]\tTest-mae:0.18965\n",
      "[728]\tTest-mae:0.18965\n",
      "[729]\tTest-mae:0.18965\n",
      "[730]\tTest-mae:0.18965\n",
      "[731]\tTest-mae:0.18965\n",
      "[732]\tTest-mae:0.18963\n",
      "[733]\tTest-mae:0.18962\n",
      "[734]\tTest-mae:0.18962\n",
      "[735]\tTest-mae:0.18961\n",
      "[736]\tTest-mae:0.18960\n",
      "[737]\tTest-mae:0.18958\n",
      "[738]\tTest-mae:0.18958\n",
      "[739]\tTest-mae:0.18958\n",
      "[740]\tTest-mae:0.18959\n",
      "[741]\tTest-mae:0.18957\n",
      "[742]\tTest-mae:0.18957\n",
      "[743]\tTest-mae:0.18956\n",
      "[744]\tTest-mae:0.18956\n",
      "[745]\tTest-mae:0.18956\n",
      "[746]\tTest-mae:0.18954\n",
      "[747]\tTest-mae:0.18953\n",
      "[748]\tTest-mae:0.18954\n",
      "[749]\tTest-mae:0.18953\n",
      "[750]\tTest-mae:0.18953\n",
      "[751]\tTest-mae:0.18954\n",
      "[752]\tTest-mae:0.18952\n",
      "[753]\tTest-mae:0.18951\n",
      "[754]\tTest-mae:0.18951\n",
      "[755]\tTest-mae:0.18951\n",
      "[756]\tTest-mae:0.18951\n",
      "[757]\tTest-mae:0.18951\n",
      "[758]\tTest-mae:0.18951\n",
      "[759]\tTest-mae:0.18950\n",
      "[760]\tTest-mae:0.18949\n",
      "[761]\tTest-mae:0.18949\n",
      "[762]\tTest-mae:0.18948\n",
      "[763]\tTest-mae:0.18947\n",
      "[764]\tTest-mae:0.18942\n",
      "[765]\tTest-mae:0.18942\n",
      "[766]\tTest-mae:0.18940\n",
      "[767]\tTest-mae:0.18940\n",
      "[768]\tTest-mae:0.18937\n",
      "[769]\tTest-mae:0.18937\n",
      "[770]\tTest-mae:0.18935\n",
      "[771]\tTest-mae:0.18935\n",
      "[772]\tTest-mae:0.18935\n",
      "[773]\tTest-mae:0.18934\n",
      "[774]\tTest-mae:0.18934\n",
      "[775]\tTest-mae:0.18932\n",
      "[776]\tTest-mae:0.18932\n",
      "[777]\tTest-mae:0.18933\n",
      "[778]\tTest-mae:0.18932\n",
      "[779]\tTest-mae:0.18932\n",
      "[780]\tTest-mae:0.18932\n",
      "[781]\tTest-mae:0.18933\n",
      "[782]\tTest-mae:0.18932\n",
      "[783]\tTest-mae:0.18931\n",
      "[784]\tTest-mae:0.18930\n",
      "[785]\tTest-mae:0.18928\n",
      "[786]\tTest-mae:0.18928\n",
      "[787]\tTest-mae:0.18929\n",
      "[788]\tTest-mae:0.18928\n",
      "[789]\tTest-mae:0.18927\n",
      "[790]\tTest-mae:0.18928\n",
      "[791]\tTest-mae:0.18926\n",
      "[792]\tTest-mae:0.18924\n",
      "[793]\tTest-mae:0.18925\n",
      "[794]\tTest-mae:0.18924\n",
      "[795]\tTest-mae:0.18923\n",
      "[796]\tTest-mae:0.18923\n",
      "[797]\tTest-mae:0.18921\n",
      "[798]\tTest-mae:0.18920\n",
      "[799]\tTest-mae:0.18920\n",
      "[800]\tTest-mae:0.18919\n",
      "[801]\tTest-mae:0.18919\n",
      "[802]\tTest-mae:0.18918\n",
      "[803]\tTest-mae:0.18919\n",
      "[804]\tTest-mae:0.18919\n",
      "[805]\tTest-mae:0.18919\n",
      "[806]\tTest-mae:0.18919\n",
      "[807]\tTest-mae:0.18918\n",
      "[808]\tTest-mae:0.18919\n",
      "[809]\tTest-mae:0.18918\n",
      "[810]\tTest-mae:0.18918\n",
      "[811]\tTest-mae:0.18918\n",
      "[812]\tTest-mae:0.18917\n",
      "[813]\tTest-mae:0.18914\n",
      "[814]\tTest-mae:0.18914\n",
      "[815]\tTest-mae:0.18914\n",
      "[816]\tTest-mae:0.18914\n",
      "[817]\tTest-mae:0.18913\n",
      "[818]\tTest-mae:0.18914\n",
      "[819]\tTest-mae:0.18912\n",
      "[820]\tTest-mae:0.18912\n",
      "[821]\tTest-mae:0.18912\n",
      "[822]\tTest-mae:0.18912\n",
      "[823]\tTest-mae:0.18910\n",
      "[824]\tTest-mae:0.18911\n",
      "[825]\tTest-mae:0.18910\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = model.best_iteration + 1\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Y_train log GB prediction with HP Tuning: 0.0962218670146364\n",
      "MAE of Y_test log GB prediction with HP Tuning: 0.18909769583180996\n"
     ]
    }
   ],
   "source": [
    "bmlog_pred_test = best_model.predict(dtest)\n",
    "bmlog_pred_train = best_model.predict(dtrain)\n",
    "\n",
    "print(\"MAE of Y_train log GB prediction with HP Tuning:\",mean_absolute_error(log_y_train, bmlog_pred_train))\n",
    "print(\"MAE of Y_test log GB prediction with HP Tuning:\",mean_absolute_error(log_y_test, bmlog_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following Metrics are after Hyper-Parameter Tuning\n",
      "R2  0.7229631108758336\n",
      "\u001b[1mMAE  191770.8591292177\u001b[0m\n",
      "MSE  225980317399.05078\n",
      "RMSE  437.91649789568066\n",
      "MAPE  0.216080021395455\n",
      "MAE of Y_train GB prediction with HP Tuning: 155064.09292400978\n",
      "MAE of Y_test GB prediction with HP Tuning: 220371.97907164492\n",
      "MAE of Y_train GB prediction with HP Tuning: 89318.758322242\n",
      "MAE of Y_test GB prediction with HP Tuning: 191770.8591292177\n"
     ]
    }
   ],
   "source": [
    "bm_pred_test = np.exp(bmlog_pred_test)\n",
    "bm_pred_train = np.exp(bmlog_pred_train)\n",
    "\n",
    "# Turning our prediction back to base numbers on which we did log\n",
    "# Turning our prediction back to base numbers on which we did log\n",
    "print('The following Metrics are after Hyper-Parameter Tuning')\n",
    "print('R2 ',r2(y_test, bm_pred_test))\n",
    "print(style.BOLD + 'MAE ', str(mae(y_test, bm_pred_test)) + style.END)\n",
    "print('MSE ',mse(y_test, bm_pred_test))\n",
    "print('RMSE ',np.sqrt(mae(y_test, bm_pred_test)))\n",
    "print('MAPE ',mape(y_test, bm_pred_test))\n",
    "\n",
    "print(\"MAE of Y_train GB prediction with HP Tuning:\",mean_absolute_error(y_train, gb_train))\n",
    "print(\"MAE of Y_test GB prediction with HP Tuning:\",mean_absolute_error(y_test, gb_pred))\n",
    "print(\"MAE of Y_train GB prediction with HP Tuning:\",mean_absolute_error(y_train, bm_pred_train))\n",
    "print(\"MAE of Y_test GB prediction with HP Tuning:\",mean_absolute_error(y_test, bm_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Best Model<a id='4.9_Best_Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_model(\"xgb_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our best model was achieved with utilizing a gradient boosting algorithm specifically we used XGBoost and we further fined tuned our hyper-parameters for our xgboost model, where we found that the following were the best parameters which lowered our prefer metric of MAE \n",
    "    - max_depth: 9\n",
    "    -min_child_weight: 3\n",
    "    -eta: 0.05\n",
    "    -subsample: 1.0\n",
    "    -colsample_bytree: 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Summary<a id='4.10_Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We started with our training and testing data from our previous notebook(*this is from a experimental notebook where the imputation was done via gradientboosting implementation of MICE and not from notebook 4 that notebook is Preprocessing and Modeling in REDO folder) and evaluated mutiple models to test which would give the best results. \n",
    "\n",
    "#### With modern algorithms such as gradient boosting we know it  would yield the best results but tried linear regression, nearest neighbor regression as well as randomforest regression while they were all outdone by gradient boosting, kNN and randomforest were not so far off. After I selected the best model being XGBoost's gradient boosting, I did further hyper-parameter tuning to squeeze a little more reduction in MAE our preffered choice of metric evaluation. This yield greater results and reduced our MAE from 220371 to 191770"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
